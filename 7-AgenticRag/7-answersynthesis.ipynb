{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e61cda",
   "metadata": {},
   "source": [
    "### 🧠 Answer Synthesis from Multiple Sources\n",
    "✅ What Is It?\n",
    "\n",
    "Answer synthesis from multiple sources is the process where an AI agent collects information from different retrieval tools or knowledge bases, and merges that information into a single, coherent, and contextually rich answer.\n",
    "\n",
    "This is a core capability in Agentic RAG, where the system is more than just a simple retriever — it plans, retrieves, and then synthesizes an answer that draws from multiple sources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e498f0",
   "metadata": {},
   "source": [
    "🎯 Why It’s Needed\n",
    "Most real-world queries are:\n",
    "- Multifaceted (require multiple types of information)\n",
    "- Ambiguous or incomplete (need refinement)\n",
    "- Open-ended (don’t map to a single document or source)\n",
    "\n",
    "🔍 This makes retrieving from a single vector DB insufficient.\n",
    "\n",
    "Instead, we want an agent that can:\n",
    "\n",
    "- Decide what to fetch from where (retrieval planning)\n",
    "- Retrieve content from multiple tools (e.g., Wikipedia, PDFs, APIs, SQL)\n",
    "- Evaluate and merge that context\n",
    "- Produce a single human-like response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb8854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders.youtube import YoutubeLoader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231639d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5597d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_retriever(file_path):\n",
    "    docs = TextLoader(file_path, encoding=\"utf-8\").load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    vs = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "    return vs.as_retriever()\n",
    "\n",
    "def load_youtube_retriever():\n",
    "    # Mocked YouTube transcript text\n",
    "    content = \"\"\"\n",
    "    This video explains how agentic AI systems rely on feedback loops, memory, and tool use.\n",
    "    It compares them to traditional pipeline-based LLMs. Temporal reasoning and autonomous tasking are emphasized.\n",
    "    \"\"\"\n",
    "    doc = Document(page_content=content, metadata={\"source\": \"youtube\"})\n",
    "    vectorstore = FAISS.from_documents([doc], OpenAIEmbeddings())\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "def wikipedia_search(query: str) -> str:\n",
    "    print(\"🌐 Searching Wikipedia...\")\n",
    "    wiki_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    return wiki_tool.invoke(query)\n",
    "\n",
    "def arxiv_search(query: str) -> str:\n",
    "    print(\"📄 Searching ArXiv...\")\n",
    "    results = ArxivLoader(query).load()\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in results[:2]) or \"No relevant papers found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568f9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_retriever = load_text_retriever(\"sample_docs.txt\")\n",
    "youtube_retriever = load_youtube_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "867eea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### state\n",
    "class MultiSourceRAGState(BaseModel):\n",
    "    question: str\n",
    "    text_docs: List[Document] = []\n",
    "    yt_docs: List[Document] = []\n",
    "    wiki_context: str = \"\"\n",
    "    arxiv_context: str = \"\"\n",
    "    final_answer: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9f025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Nodes\n",
    "def retrieve_text(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    docs = text_retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"text_docs\": docs})\n",
    "\n",
    "def retrieve_yt(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    docs = youtube_retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"yt_docs\": docs})\n",
    "\n",
    "def retrieve_wikipedia(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    result = wikipedia_search(state.question)\n",
    "    return state.model_copy(update={\"wiki_context\": result})\n",
    "\n",
    "def retrieve_arxiv(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    result = arxiv_search(state.question)\n",
    "    return state.model_copy(update={\"arxiv_context\": result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed172aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## synthesize\n",
    "def synthesize_answer(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    \n",
    "    context = \"\"\n",
    "\n",
    "    context += \"\\n\\n[Internal Docs]\\n\" + \"\\n\".join([doc.page_content for doc in state.text_docs])\n",
    "    context += \"\\n\\n[YouTube Transcript]\\n\" + \"\\n\".join([doc.page_content for doc in state.yt_docs])\n",
    "    context += \"\\n\\n[Wikipedia]\\n\" + state.wiki_context\n",
    "    context += \"\\n\\n[ArXiv]\\n\" + state.arxiv_context\n",
    "\n",
    "    prompt = f\"\"\"You have retrieved relevant context from multiple sources. Now synthesize a complete and coherent answer.\n",
    "\n",
    "Question: {state.question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "\n",
    "    answer = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update={\"final_answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6201f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000023762B09C40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = StateGraph(MultiSourceRAGState)\n",
    "\n",
    "builder.add_node(\"retrieve_text\", retrieve_text)\n",
    "builder.add_node(\"retrieve_yt\", retrieve_yt)\n",
    "builder.add_node(\"retrieve_wiki\", retrieve_wikipedia)\n",
    "builder.add_node(\"retrieve_arxiv\", retrieve_arxiv)\n",
    "builder.add_node(\"synthesize\", synthesize_answer)\n",
    "\n",
    "builder.set_entry_point(\"retrieve_text\")\n",
    "builder.add_edge(\"retrieve_text\", \"retrieve_yt\")\n",
    "builder.add_edge(\"retrieve_yt\", \"retrieve_wiki\")\n",
    "builder.add_edge(\"retrieve_wiki\", \"retrieve_arxiv\")\n",
    "builder.add_edge(\"retrieve_arxiv\", \"synthesize\")\n",
    "builder.add_edge(\"synthesize\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0789c928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Searching Wikipedia...\n",
      "📄 Searching ArXiv...\n",
      "✅ Final Answer:\n",
      "\n",
      "Transformer agents refer to advanced AI systems, particularly those employing the transformer architecture, that function interactively within a defined agent loop of thinking, acting, and observing. These agents utilize large language models (LLMs) such as OpenAI's GPT series, which are pre-trained on substantial datasets and fine-tuned for various applications. In recent research, the evolution of transformer agents has focused on enhancing their capabilities through improved interaction with tools, better handling of complex tasks, and boosting their reasoning abilities.\n",
      "\n",
      "### Key Features of Transformer Agents:\n",
      "\n",
      "1. **Agent Loop**: At the core of transformer agents is the agent loop which allows these systems to continuously refine their actions based on outcomes observed from their environment. This cyclical process is crucial for real-time adaptation in diverse and dynamic scenarios, enabling agents to improve performance incrementally.\n",
      "\n",
      "2. **Memory and Planning**: Transformer agents can retain information from past interactions (memory) and make informed plans for future actions. This feature equips them to handle more nuanced queries that require a deeper understanding of context and past inputs.\n",
      "\n",
      "3. **Tool Interaction**: Modern transformer agents are designed to interact with various tools such as web browsers, databases, and APIs. This interaction enables them to retrieve real-time information and perform tasks that extend beyond mere text generation, akin to using specialized software applications.\n",
      "\n",
      "4. **Complex Task Management**: Newer transformer agents are capable of managing open-ended questions that require complex reasoning. Research has introduced datasets like \"Researchy Questions,\" which focus on multi-faceted, non-factoid queries that challenge LLMs like GPT-4 in handling \"unknown unknowns\"—aspects of problems that may not be immediately clear or visible.\n",
      "\n",
      "5. **Decomposition Techniques**: Recent advancements in processing complex questions involve breaking them down into sub-questions. Techniques such as chain-of-thought reasoning are employed, where the AI considers how to reach the answer step-by-step rather than attempting to solve it in one go. This method enhances the accuracy and depth of responses.\n",
      "\n",
      "6. **Evaluating Agent Performance**: Research has also shifted towards evaluating transformer agents based on their ability to generate accurate responses to complex queries, with benchmarks being developed that assess reasoning, factuality, and alignment with human expectations. Models are trained using methods such as Reinforcement Learning from Human Feedback (RLHF) to ensure that responses meet user needs effectively.\n",
      "\n",
      "### Recent Research Directions:\n",
      "\n",
      "- **Emergence of Researchy Questions**: The \"Researchy Questions\" dataset has emerged as essential for evaluating how transformer agents can decompose complex queries into manageable parts. Designed to challenge AI systems, it encourages the development of techniques that enable deeper understanding and response synthesis across diverse domains.\n",
      "\n",
      "- **Agentic AI Frameworks**: The introduction of frameworks that allow LLMs to function as agents in iterative cycles has led to the development of tools that mimic human-like interaction patterns with information retrieval systems.\n",
      "\n",
      "- **Real-World Application Integration**: Many contemporary studies are exploring how transformer agents can be leveraged in practical applications, such as enhancing customer service, conducting research, and supporting decision-making processes through effective AI-human collaboration.\n",
      "\n",
      "In summary, transformer agents are evolving rapidly, employing complex feedback loops, improved reasoning capabilities, and enhanced interaction with external tools. Ongoing research focuses on addressing their limitations in real-world applications and refining their ability to solve intricate problems through advanced data structures and cognitive techniques.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are transformer agents and how are they evolving in recent research?\"\n",
    "state = MultiSourceRAGState(question=question)\n",
    "result = graph.invoke(state)\n",
    "\n",
    "print(\"✅ Final Answer:\\n\")\n",
    "print(result[\"final_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88402b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are transformer agents and how are they evolving in recent research?',\n",
       " 'text_docs': [Document(id='b8c6b78d-97c7-49a1-a83b-f0730c49ff99', metadata={'source': 'sample_docs.txt'}, page_content='An agent loop is the cycle of thinking, acting, and observing. It allows an AI agent to continuously refine its actions based on results.\\nThis feedback loop is crucial in autonomous systems to adapt in real-time.\\nAgent-based architectures benefit from memory, planning, and interaction with tools.')],\n",
       " 'yt_docs': [Document(id='3d52ff38-14b4-485a-8b0b-46320cb3b128', metadata={'source': 'youtube'}, page_content='\\n    This video explains how agentic AI systems rely on feedback loops, memory, and tool use.\\n    It compares them to traditional pipeline-based LLMs. Temporal reasoning and autonomous tasking are emphasized.\\n    ')],\n",
       " 'wiki_context': 'Page: Generative pre-trained transformer\\nSummary: A generative pre-trained transformer (GPT) is a type of large language model (LLM) that is widely used in generative AI chatbots. GPTs are based on a deep learning architecture called the transformer. They are pre-trained on large datasets of unlabeled content, and able to generate novel content.\\nOpenAI was the first to apply generative pre-training (GP) to the transformer architecture, introducing the GPT-1 model in 2018. The company has since released many bigger GPT models. The popular chatbot ChatGPT, released in late 2022 (using GPT-3.5), was followed by many competitor chatbots using their own \"GPT\" models to generate text, such as Gemini, DeepSeek or Claude.\\nGPTs are primarily used to generate text, but can be trained to generate other kinds of data. For example, GPT-4o can process and generate text, images and audio. To improve performance on complex tasks, some GPTs, such as OpenAI o3, spend more time analyzing the problem before generating an output, and are called reasoning models. In 2025, GPT-5 was released with a router that automatically selects whether to use a faster model or slower reasoning model based on task.\\n\\n\\n\\nPage: Large language model\\nSummary: A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. The largest and most capable LLMs are generative pre-trained transformers (GPTs) and provide the core capabilities of chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\\nThey consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents, code generation, knowledge retrieval, and automated reasoning that previously required bespoke systems.\\nLLMs evolved from earlier statistical and recurrent neural network approaches to language modeling. The transformer architecture, introduced in 2017, replaced recurrence with self-attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. This innovation enabled models like GPT, BERT, and their successors, which demonstrated emergent behaviors at scale such as few-shot learning and compositional reasoning.\\nReinforcement learning, particularly policy gradient algorithms, has been adapted to fine-tune LLMs for desired behaviors beyond raw next-token prediction. Reinforcement learning from human feedback (RLHF) applies these methods to optimize a policy, the LLM\\'s output distribution, against reward signals derived from human or automated preference judgments. This has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance.\\nBenchmark evaluations for LLMs have evolved from narrow linguistic assessments toward comprehensive, multi-task evaluations measuring reasoning, factual accuracy, alignment, and safety. Hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements.\\n\\n\\n\\nPage: AI safety\\nSummary: AI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence (AI) systems. It encompasses AI alignment (wh',\n",
       " 'arxiv_context': 'Researchy Questions: A Dataset of Multi-Perspective, Decompositional\\nQuestions for LLM Web Agents\\nCorby Rosset\\nMicrosoft\\nHo-Lam Chung\\nNational Taiwan Univ.\\nGuanghui Qin\\nJohns Hopkins Univ.\\nEthan C. Chau\\nMicrosoft\\nZhuo Feng\\nMicrosoft\\nAhmed Awadallah\\nMicrosoft\\nJennifer Neville\\nMicrosoft\\nNikhil Rao\\nMicrosoft\\nAbstract\\nExisting question answering (QA) datasets are\\nno longer challenging to most powerful Large\\nLanguage Models (LLMs). Traditional QA\\nbenchmarks like TriviaQA, NaturalQuestions,\\nELI5 and HotpotQA mainly study “known un-\\nknowns” with clear indications of both what\\ninformation is missing, and how to find it to\\nanswer the question. Hence, good performance\\non these benchmarks provides a false sense\\nof security.\\nA yet unmet need of the NLP\\ncommunity is a bank of non-factoid, multi-\\nperspective questions involving a great deal\\nof unclear information needs, i.e. “unknown\\nuknowns”. We claim we can find such ques-\\ntions in search engine logs, which is surprising\\nbecause most question-intent queries are indeed\\nfactoid. We present Researchy Questions, a\\ndataset of search engine queries tediously fil-\\ntered to be non-factoid, “decompositional” and\\nmulti-perspective. We show that users spend\\na lot of “effort” on these questions in terms\\nof signals like clicks and session length, and\\nthat they are also challenging for GPT-4. We\\nalso show that “slow thinking” answering tech-\\nniques, like decomposition into sub-questions\\nshows benefit over answering directly. We re-\\nlease1 ∼100k Researchy Questions, along with\\nthe Clueweb22 URLs that were clicked.\\n1\\nIntroduction\\nThe advent of Large Language Models (LLMs) has\\nushered in a new era for the field natural language\\nprocessing, with short- and long- form question-\\nanswering (QA) at the forefront of recent achieve-\\nments (OpenAI et al., 2023). Historically, QA\\nbenchmarks served as the crucible for evaluating a\\nmodel’s grasp of natural language understanding.\\nHowever, LLMs have nearly perfected many QA\\ndatasets, particularly those involving answering\\nshort, factual questions like “What is the capital\\n1https://huggingface.co/datasets/corbyrosset/\\nresearchy_questions\\nDataset\\nNum. Topics Sub-Ques. Sub-Query\\nHotpot QA\\n300\\n2.9\\n3.8\\n3.6\\nOpenBook QA\\n300\\n3.8\\n6.3\\n5.9\\nStrategy QA\\n300\\n3.8\\n5.3\\n4.9\\nTruthful QA\\n300\\n3.8\\n6.4\\n6.0\\nAquamuse\\n300\\n3.7\\n5.4\\n5.2\\nReddit/askh\\n300\\n4.9\\n9.4\\n8.5\\nReddit/asks\\n300\\n5.1\\n9.2\\n8.8\\nReddit/eli5\\n300\\n4.5\\n9.7\\n9.3\\nStack Exchange\\n300\\n6.1\\n8.4\\n7.6\\nWikihow\\n300\\n4.8\\n11.7\\n11.2\\nResearchy\\n96k\\n3.9\\n14.3\\n12.6\\nTable 1: We asked GPT-4 to decompose questions into\\na natural hierachy of subquestions, as well as concrete\\nqueries it would issue to a search engine. As expected,\\nHotpotQA requires the fewest sub-questions to answer.\\nof Brunei?\". The re-emergence of chatbots and\\n“agentic” AI assistants, which represent complex\\nsystems based on LLMs as a building block, have\\nled to new opportunities for users to ask deeper,\\nmore nuanced questions as shown in Figure 1.\\nHowever, the capabilities of powerful AI-\\nassisted QA tools has outpaced the metrics needed\\nto evaluate them. A number of traditional QA\\nbenchmarks like Natural Questions (Kwiatkowski\\net al., 2019), TriviaQA (Joshi et al., 2017), We-\\nbQuestions (Berant et al., 2013), SearchQA (Dunn\\net al., 2017) have been more or less solved by mod-\\nern LLMs. These datasets consist primarily of\\nfactoid questions (from search engine logs, Trivia,\\nJeopardy!, etc), where answers are typically found\\nwithin a single sentence or paragraph that almost\\ncertainly exist in standard pre-training web cor-\\npora (Zhou et al., 2023). For instance, MS Marco\\n(Campos et al., 2016) is up to 55% factoid ques-\\ntions (Bolotova et al., 2022). The obvious draw-\\nback to these kinds of QA datasets is that the an-\\nswers can be memorized by LLMs or solved with\\nsimple pattern-matching or keyword-based search.\\nMulti-hop reasoning tasks including HotpotQA\\n(Yang et al., 2018), HybridQA (Chen et al., 2020),\\nMuSiQue (Trivedi et al., 2022) aim to challenge\\narXiv:2402.17896v1  [cs.CL]  27 Feb 2024\\nFigure 1: Qualitative comparison of how Researchy Questions differs from other Question Answering datasets.\\nResearchy Questions involve a greater deal of complexity and “unknown unknowns” than other QA datasets.\\nQA systems to logically bridge information across\\nmultiple documents or paragraphs. While these\\ndatasets have made strides in increasing question\\ncomplexity, the answers are still ultimately factoid,\\nand it’s clear what sub-questions ought to be asked\\nto recall the missing information. Furthermore, the\\nconstruction of these datasets (e.g. synthetically\\ngenerated from paths of wikipedia links) leads to a\\ndistribution mismatch w.r.t questions humans ask.\\nThere exist several sources of long-form, non-\\nfactoid QA datasets like ELI5 (Fan et al., 2019),\\nStack Exchange, Yahoo Answers (Zhang et al.,\\n2016), and WikiHowQA (Bolotova-Baranova et al.,\\n2023). While answers to these questions are more\\ninvolved than factoid questions, ELI5 and Wiki-\\nHowQA elicit more expository answers than analyt-\\nical ones. The Oxford Allsouls dataset (Liu et al.,\\n2023b) contains 1k college-level essay prompts\\nwhich are multi-perspective, but are designed to\\nevaluate persuasive writing skills and do not have\\nassociated documents to ground responses. AQua-\\nMuSe (Kulkarni et al., 2020) is an excellent at-\\ntempt at filtering Natural Questions (NQ) for multi-\\nfaceted queries, but their method was constrained\\nby relying on the relatively short paragraph-length\\nanswers already present in NQ.\\nThe rise of “LLM Agents” e.g.\\n(Wu et al.,\\n2023b) have opened the door for even deeper col-\\nlaboration between users, LLMs, and tools. In\\nresponse, more recent datasets have focused on\\ncompleting challenging tasks in open-ended envi-\\nronments with tools like a web browser, file system,\\ndatabase, etc.\\nIn particular, Gaia (Mialon et al., 2023) tests\\nunderstanding of multi-modal inputs (images and\\ntext), as well as complex reasoning across levels of\\ndifficulty to solve. AgentBench (Liu et al., 2023c)\\nprovides closed environments for an LLM to in-\\nteract with APIs in various scenarios, including\\ncoding (interacting with file systems or databases),\\ngames/puzzles, and web browsing/shopping. While\\nthese datasets advance the field of metrics for LLM\\nagents, they are small, consisting of only 466 and\\n1,091 questions respectively that were hand-curated\\nby the authors.\\nThe need for more challenging QA datasets also\\nstems from some alarming trends: while there are\\nhundreds of public LLMs, they are pretrained on\\nonly a handful existing corpora (Gao et al., 2020;\\nRaffel et al., 2023), or distilled from one of a\\nfew teacher LLMs (Peng et al., 2023). Further-\\nmore, much more data scraped from the internet\\nfor training is itself going to be AI-generated con-\\ntent, leading to an echo-chamber effect (Dohma-\\ntob et al., 2024; Wu et al., 2023a). Hence, the\\nconvergent evolution (Stayton, 2015) of LLMs\\nincreases the risk that they will not recognize\\nthat they don’t know something, e.g. see GPT-\\n4 and Mixtral 8x7b in Table 10.\\nThis is espe-\\ncially true when LLMs act in the capacity of\\nLLM-as-a-judge (Zheng et al., 2023a; Yuan et al.,\\n2024), or when faced with very multi-faceted/multi-\\nResearchy Question: how does public transport help the economy\\nHierarchical Decomposition of the Question\\nClicked Clueweb22 URLs\\n1. What is public transport?\\n(a) What are the different types of public transport?\\n(b) How many people use public transport in different regions or countries?\\n2. What are the direct economic benefits of public transport?\\n(a) How public transport reduces costs for users e.g. fuel, parking, maintenance, etc.?\\n(b) How does public transport generate revenue...\\n3. What are the indirect economic benefits of public transport?\\n(a) How does public transport reduce congestion...\\n(b) How does public transport increase access to education, employment, health, ...\\n(c) How does public transport enhance productivity and innovation...\\n(d) How does public transport contribute to environmental and social goals...\\n4. How do the economic benefits of public transport compare to the costs of providing and\\nmaintaining public transport?\\n(a) What are the main costs of public transport...\\n(b) How are the costs of public transport financed...\\n(c) How are the benefits and costs of public transport measured and evaluated...\\n1. infrastructureusa.org\\n2. nationalgeographic.org\\n3. quora.com\\n4. accessmagazine.org\\n5. ced.berkeley.edu\\n6. greenertransportsolutions\\n7. bts.gov\\n8. apta.com\\nExample of a Pivotal Fact from URL: accessmagazine.org\\n“...Even in cities with a high concentration of office space in the [central business district], we estimate that increasing transit\\nridership by 10 percent will increase office rents by no more than 0.5 percent. For all other cities, we estimate that increasing\\ntransit ridership will have no effect on office rents...”\\nTable 2: An example Researchy Question, how GPT-4 decomposed it into sub-questions (closed-book), as well the\\nURLs in ClueWeb22 that real users clicked on, and an example of a Pivotal Fact from one of those URLs.\\nperspective questions, where the consequences\\ncould be that users “miss the whole picture” or\\nworse, are misled (Zheng et al., 2023b; Liu et al.,\\n2023b). While retrieval-augmentation (Lewis et al.,\\n2021; Borgeaud et al., 2022; Guu et al., 2020) can\\nhelp complement LLM agents, the risk is merely\\nshifted to whether a sub-system retrieves the right\\ninformation and uses it correctly (Liu et al., 2023a).\\nWe believe the well-studied phenomenon of\\n“unknown unknowns” (United States Congress\\net al., 1981) applies to LLM Agents in scenarios ad-\\ndressing complex questions requiring “slow think-\\ning” (Kahneman, 2011). Simply put, one strategy\\nis to iteratively re-frame or decompose the problem\\ninto a set of “known unknowns” (which charac-\\nterize most of the aforementioned QA datasets).\\nFor these sub-problems, it should be clearer what\\ninformation is missing, how to find it, and once\\nfound, how the “known known” contributes to the\\nfinal answer. Several techniques such as chain-\\nof-thought question decomposition (Radhakrish-\\nnan et al., 2023) and tree-of-thought (Yao et al.,\\n2023a) prompting take a similar approach to plan\\nlong-horizon solutions to complex problems. How-\\never, those studies still operate over traditional QA\\nbenchmarks like HotpotQA, or over simple games\\nlike crossword puzzles. Hence, the right bench-\\nmark of questions for these advanced decomposi-\\ntion techniques still does not exist for open-domain\\nweb scenarios (Krishna et al., 2021).\\nWe present Researchy Questions to study the\\ndynamics of how LLM agents handle unclear infor-\\nmation needs associated with very complex ques-\\ntions. We define a Researchy Question as a non-\\nfactoid question that expects a long-form answer\\n(longer than a paragraph!) entailing substantial\\nresearch or effort to synthesize.\\nA Researchy\\nQuestion can be instantiated as a complex search\\ntask (Aula and Russell, 2008) with unclear infor-\\nmation needs that requires analyzing multiple docu-\\nments or pieces of evidence. A Researchy Question\\ndoes not have a single correct answer, but rather\\nmultiple perspectives allowing a dense manifold of\\nanswers over which varying criteria can determine\\nwhich is better. In practice, the act of answering\\na Researchy Question probably involves decompo-\\nsition into sub-questions that aid the retrieval of\\ncomprehensive information, reducing the risk of\\nmissing unknown unknowns. Lastly, a Researchy\\nQuestion represents a genuine information need\\nthat real people asked. Figure 1 qualitatively com-\\npares other canonical QA datasets.\\nResearchy\\nQuestions is primarily a QA\\ndataset to evaluate question answering systems or\\nLLM Agents with the ultimate goal of achieving\\never higher-quality answers using any tools neces-\\nsary. Notwithstanding, it is also a search/retrieval\\ndataset in the sense that finding and correctly incor-\\nFirst MS Marco\\nQnA\\nNon-factoid Researchy (↓)\\nhow 17.0%\\n34.3%\\n29.4%\\n41.2%\\nwhy 1.64%\\n6.26%\\n33.4%\\n22.9%\\nwhat 34.9%\\n12.2%\\n21.3%\\n19.1%\\nis/are/do 5.77%\\n15.0%\\n6.50%\\n9.67%\\nshould 0.11%\\n0.53%\\n0.59%\\n1.85%\\ncan 1.84%\\n4.31%\\n1.01%\\n0.97%\\nwho 3.27%\\n4.77%\\n0.90%\\n0.47%\\nwhich 1.78%\\n2.78%\\n1.87%\\n0.46%\\nwhen 2.70%\\n5.03%\\n0.44%\\n0.43%\\npros < 0.01%\\n0.03%\\n0.45%\\n0.28%\\nexplain 0.05%\\n0.06%\\n0.12%\\n0.23%\\nwhere 3.54%\\n4.08%\\n0.37%\\n0.17%\\nfactors < 0.01%\\n0.01%\\n0.08%\\n0.15%\\nwill 0.10%\\n0.69%\\n0.08%\\n0.15%\\ndescribe 0.05%\\n0.07%\\n0.04%\\n0.09%\\nTable 3: Top most-prevalent first-words of the three\\nmain stages of our question filtering funnel (with MS\\nMarco for comparison).\\nporating the right evidence is a critical sub-system\\nto meet expectations of trustworthiness and ground-\\nedness (Zheng et al., 2023b; Liu et al., 2023b).\\nWhile we believe that question decomposition is a\\ncritical piece of solving Researchy Questions ,\\nit is unclear how to define or measure the quality\\nof sub-questions. To aid this endeavor, we reveal\\nwhat URLs end users found useful, with the hope\\nthat good sub-questions will at least lead to the\\ninformation found in those clicked documents.\\nWe\\nrelease\\nabout\\n96K\\nResearchy\\nQuestions consisting of real users’ queries\\nto a commercial search engine, and additionally:\\n1. The decomposition of the question into a 2-\\nlevel hierarchical plan (See Table 2 Left).\\n2. For each question, the user-aggregated click\\ndistribution over URLs in a publicly available\\nweb corpus, ClueWeb22.\\n3. Ordered list of sub-queries corresponding\\nroughly to the sub-questions that can be read-\\nily issued to a search engine\\nIn Section 2 we describe how Researchy\\nQuestions were obtained and then characterize\\nthem in Section 3.\\nIn Section 4 we verify\\nthat web users expend more effort on Researchy\\nQuestions over other queries. In Section 5 we\\nevaluate and compare decompositional answering\\ntechniques from (Radhakrishnan et al., 2023).\\n2\\nResearchy Questions Construction\\nResearchy Questions are real user queries from\\nsearch logs. While search logs have a rich diversity\\nof query types and intents (Bolotova et al., 2022;\\nBu et al., 2010), they mostly contain factoid or\\nnavigational queries, which need to be filtered.\\n2.1\\nStage 1: Mining Search Logs\\nWe obtained a set of query-url click pairs from\\na commercial search engine that were logged be-\\ntween July 2021 and August 2022, which maxi-\\nmally overlaps with the creation of the Clueweb22\\nsnapshot of web documents (Overwijk et al., 2022).\\nThis way, we can simply indicate which Researchy\\nQuestions clicked on which documents. We be-\\ngan with a large sample of english, non-adult\\nqueries that had least one click. We denote these as\\n“General Queries”, which were further filtered.\\nAn important filtering criteria is frequency: we\\nkept queries if they were issued at least 50 times\\nin the logs. This criteria is simple but powerful: it\\nhelps denoise the dataset (reduces spelling errors),\\nand also allows us to focus on questions that are\\nnot “one-offs”. This helps us glean insights into\\nrepeated user behaviors as they interact with the\\nsearch engine.\\nIn order to select queries which are answer-\\nseeking intent (i.e. actual “questions” as distinct\\nfrom navigational queries like “facebook login”,\\nshopping intent like “fast running shoes”, or local\\nintent like “best pizza near me”), we used a suite\\nof rules and existing production classifiers:\\n• Query Language: English\\n• Adult intent: False\\n• Number of Distinct Occurrences: ≥50\\n• 3 ≤Number of Query Words: ≤15\\n• Number of Distinct URLs Clicked On: ≥2\\n• Question Intent Classifier: True\\n• Navigational Intent: False\\n• Local / Real Estate / Map Intent: False\\n• Retail/Shopping Intent: False\\n• Coding/Technical Intent: False\\n• Health/Medical Intent: False\\n• Triggered Possible Answer Cards: ≥1\\n• Triggered lots of Ads: False\\nTo explain some of the points above: an Answer\\nCard is a high-precision feature in a search engine\\nwhere a paragraph containing the answer is shown\\nat the top of the results page, distinct from the “ten\\nblue links”. Because search engines are constantly\\nbeing updated with new features and triggering\\nrules, all the above statistics are aggregated and\\nnormalized over the full year period. For instance,\\n“a lot of Ads” is determined by summing the total\\nnumber of ads shown for the query over the full\\nyear and dividing by the number of times the query\\nwas issued, and then picking a threshold above\\nwhich queries seemed “shopping intent”. The Ads\\nQuery Type\\nCount Avg. unique\\nclicked urls\\nAvg.\\nclicks\\nAvg. sat.\\nclicks\\nGeneral Queries\\n≥1B\\n1.88\\n4.83\\n2.54\\nQnA Queries\\n15.7M\\n3.99\\n9.31\\n5.10\\nNon-factoid QnA\\n1.0M\\n4.20\\n8.99\\n4.86\\nResearchy Queries\\n100k\\n6.31\\n15.85\\n8.54\\nSession Type\\nCount\\n# Turns # unique.\\nqueries\\n# sat\\nclicks\\nGeneral Sessions\\n≥10B\\n2.42\\n2.11\\n0.76\\nQnA Sessions\\n≥100M\\n6.28\\n5.53\\n1.15\\nN.F. QnA Sessions\\n≥10M\\n12.89\\n11.33\\n1.91\\nResearchy Sessions\\n≥1M\\n13.45\\n11.81\\n2.46\\nTable 4: Our query filtering funnel; each row is a subset from the row above it. (Left) Researchy Questions have\\nmuch more clicks (Sat clicks have longer dwelltime), and require more unique documents; the full distribution is in\\nFigure 2. (Right) Search statistics per session in which each type of query appeared, indicating that harder questions\\nappear in longer sessions. These behaviors confirm our filtering yields more complex questions.\\nrequirement also helped catch any shopping intent\\nqueries missed by the Retail intent classifier.\\nWe wanted to remove Coding/Technical queries\\nbecause such questions are often motivated by very\\nspecific problems that are often solved by one piece\\nof documentation after lots of clicking around,\\nwhich is not the behavior we wish to target in this\\ndataset. Health and medical questions are avoided\\nlargely because they often overlap too much with\\nthose that ought to be addressed by a licensed med-\\nical professional. Many shopping / retail queries\\ncould be construed as “researchy” e.g. “what are\\nthe best headphones”, but we avoid them in this\\ndataset because it is hard to distinguish whether a\\nURL was clicked due to aggressive advertising or\\na real information need.\\nAfter this stage of filtering, we arrived at 15.7M\\n“QnA Queries” which are largely recognizable as\\nnatural language questions that deal with open-\\ndomain knowledge. This size was manageable\\nenough to efficiently run our own bert-large scale\\nclassifiers we use in the next stage of filtering.\\n2.2\\nStage 2: Factoid Classifier\\nWe needed a way to distinguish which QnA Queries\\nwere factoid vs.\\nnon-factoid; we trained a bi-\\nnary classifier on auto-labeled data for this pur-\\npose. The training data was a uniform sample of\\n200k questions from the 15.7M QnA Queries. La-\\nbels for the questions were collected from gpt3\\n(text-davinci-003) prompted with few-shot ex-\\namples as shown in Figure 4. The labels were then\\nused to train a bert-large non-factoid question\\nclassifier, which was then inferenced on the full set\\nof 15.7M queries. By manual inspection, a thresh-\\nold was chosen above which we were satisfied that\\nthe questions were meaningfully non-factoid. The\\nresulting 1.0M met the non-factoid threshold of\\n0.75 as shown on the left hand side of Figure 3,\\nwhich we denote as ”Non-factoid QnA Queries”.\\n2.3\\nStage 3: Decompositional Classifier\\nNot all the resulting non-factoid QnA queries ex-\\nhibited the “decompositional” information needs.\\nNamely, they often looked expository or “how-to”\\nwith generally one correct answer which did not\\nhave many perspectives. We trained a second classi-\\nfier to score the extent to which a question requires\\nasking sub-questions. The specific definition of “re-\\nquiring sub-questions” is described in a prompt in\\nFigure 5 given to ChatGPT (gpt-35-turbo) to col-\\nlect labels. We used ChatGPT because we figured\\nthis was a relatively intense cognitive task. We\\ninferenced ChatGPT on about 40k outputs from\\nthe non-factoid classifier that met the non-factoid\\nthreshold of 0.75. We used the labels to train a\\nseparate bert-large “decompositional” classifier.\\nAgain, we chose a threshold by manual inspec-\\ntion to indicate which of the 1.0M Non-factoid\\nQnA Queries were also decompositional, which\\nhappened to be 0.6 as shown on the right-hand\\nside of Figure 3. Out of the 1.0M queries that met\\nthe non-factoid threshold, 146k also met the de-\\ncompositional threshold. These 146k became the\\nResearchy Questions candidates before dedupli-\\ncation. This procedure is not perfect; in Table 6 we\\nshow examples of some non-factoid questions that\\ndid not meet the decompositional threshold.\\n2.4\\nStage 4: Deduplication\\nThe final step of filtering is deduplication. We\\ntake an agglomerative clustering approach (Everitt,\\n1974), where the only parameter is a distance\\nthreshold ϵ below which two queries are consid-\\nered “duplicate intent”.\\nWe represent the semantic intent of a query by an\\nANCE-based (Xiong et al., 2020) vector encoder\\n−→\\nqi ←encoder(qi). We instantiate a metric space\\ndefined by 1 −cosine(·, ·) of the vector encod-\\nings using the faiss implementation of an approxi-\\nmate nearest neighbor (ANN) index (Johnson et al.,\\n2019). For each question in the index, we search\\nfor the nearest neighbors {qj ∼ANN(qi) s.t. 1.0\\n- −→\\nqi · −→\\nqj < ϵ}. For agglomerative clustering, we\\ndefine a “group” as a set of queries in which all pair-\\nwise distances are within ϵ. We found that about\\n63% of queries were singletons (did not belong to\\na group of size greater than one), and the average\\ngroup size was 3.8. For example, the queries “what\\nwere tanks used for in ww1”, “how were the tanks\\nused in ww1” and “why were tanks needed in ww1”\\nwere all part of the same group. For all groups of\\nsize greater than one, we selected the query that\\nwas issued most often in the logs as the representa-\\ntive “head” of the group. After combining the heads\\nof groups and singletons, about 70% of queries re-\\nmained, yielding 102k Researchy Questions .\\nAlthough we did our best to de-duplicate question\\nintent, some clusters of topics remain, e.g. a quick\\nkeyword count shows about 600 contain the string\\n“ww2”, or about 80 for “supreme court”.\\n2.5\\nStage 5: Final GPT-4 Filtering\\nAs a final quality control step after deduplication,\\nwe had all 102k questions labeled by GPT-4 for\\nintrinsic attributes of the question like how multi-\\nfaceted it is, how reasoning-intensive it is likely\\nto be, etc. The full set of eight attributes is defined\\nin Figure 7, along with histograms of those scores\\nin Figure 6 for both Researchy Questions and\\nNatural Questions. All eight are scored on a scale\\nof 1-10. About 3% of 102k questions were re-\\nmoved based on the attributes “ambiguous” and\\n“incomplete” which indicate defective questions\\ntoo difficult to answer; some examples are shown\\nin Table 7. Another 2% were removed for being too\\n“assumptive”, meaning the question was phrased\\nin a presumptuous way that was likely to bias the\\nanswer as shown in Table 8. Another 2% was\\nremoved for safety reasons as shown in Table 9,\\nwhere we deem risk of harm too high by attempt-\\ning to answer the question. Not all “assumptive”\\nquestions are harmful per se. Finally, the remaining\\n96k queries are what we release.\\n3\\nCharacterizing Researchy Questions\\nAcross the 96k Researchy Questions (split into\\n90k train, 6.4k test), there are in aggregate 350k\\nunique documents clicked, 48% of which can be\\nfound in the english subset of Clueweb22 Set\\nB (Overwijk et al., 2022); the rest are in Sets A\\nor L. For each question we release, there are on av-\\nerage 4.9 +/- 3.5 clicked documents (See Figure 2\\nRight), indicating a good diversity of information\\nneeds and much higher than the average query over-\\nall. Conversely, for each document there is only 1.4\\n+/- 2.3 associated Researchy Questions (See Fig-\\nure 2 Left) indicating good query de-duplication.\\nTo get a sense of how intrinsically difficult\\nResearchy\\nQuestions are compared to other\\ndatasets, we asked GPT-4 how many sub-questions\\nor search engine queries would be necessary to\\nask/issue in order to fully answer each question. An\\nexample decomposition in shown in Table 2, and\\nthe aggregate results are shown in Table 1. Clearly,\\nGPT-4 thinks that most factoid QA datasets (top\\nof the table) require the fewest sub-questions to\\nanswer, whereas Researchy Questions require\\nthe most even among longer-form QA datasets.\\nWe also compared Researchy Questions to\\nanother search-log based QA dataset – Natural\\nQuestions (Kwiatkowski et al., 2019) – along the\\n8 quality dimensions descried in Section 2.5, such\\nas how reasoning- and knowledge-intense they\\nare. The comparative histograms are shown in\\nFigure 6, Clearly, GPT-4 thinks that Researchy\\nQuestions require much more knowledge, reason-\\ning, and are inherently more multi-faceted.\\nTable 3 shows the distribution of first words\\nacross Researchy Questions (as well as inter-\\nmediate datasets used in the filtering funnel de-\\nscribed in the next section). For comparison, MS\\nMarco queries (also from web search logs) are\\nmuch more factoid – e.g. only 1.64% of them\\nstart with “why” (Bajaj et al., 2018).\\nLastly, an emergent property of Researchy\\nQuestions we observed is that some information\\nfound in the clicked URLs is very surprising, e.g.\\nfor the question “should the death penalty be legal-\\nized”, the fact that in the US, “the cost of enforcing\\nthe death penalty cost millions more than life im-\\nprisonment” 2 would have a large impact on the\\neconomic arguments in that answer. We define\\na Pivotal Fact as a piece of information that is\\nso surprising and consequential that, once known,\\ndrastically changes how an LLM Agent would an-\\nswer the question (and the answer wouldn’t be as\\ngood without it); but it wouldn’t know about it un-\\nless it asked the right sub-question to retrieve it, e.g.\\n“does the death penalty cost more than life impris-\\nonment”. Hence, a pivotal fact is the information\\nanalogue of a black swan, a critical event that is\\nhard to predict but highly impactful (Taleb, 2008);\\n2supremecourt.gov/opinions\\nMethod\\nSamples\\nDirect Answer\\nCoT decomp.\\nFactor decomp.\\nDecomp.\\nAcc.\\nScore\\nAcc.\\nScore\\nAcc.\\nScore\\nScore gain\\nHotpot QA\\n300\\n0.843\\n83.4\\n0.877\\n83.5\\n0.837\\n81.3\\n+0.1\\nOpenBook QA\\n300\\n0.926\\n86.1\\n0.843\\n83.5\\n0.750\\n80.7\\n-2.6\\nStrategy QA\\n300\\n0.757\\n80.8\\n0.810\\n83.7\\n0.777\\n82.6\\n+2.9\\nTruthful QA\\n300\\n0.703\\n73.7\\n0.789\\n82.4\\n0.739\\n81.5\\n+8.7\\nAquamuse\\n300\\n0.916\\n83.0\\n0.940\\n84.9\\n0.926\\n85.0\\n+2.0\\nReddit/askh\\n300\\n0.759\\n79.8\\n0.736\\n77.3\\n0.732\\n79.3\\n-0.5\\nReddit/asks\\n300\\n0.783\\n81.1\\n0.743\\n79.4\\n0.796\\n82.7\\n+1.6\\nReddit/eli5\\n300\\n0.883\\n83.1\\n0.890\\n85.0\\n0.890\\n86.5\\n+3.4\\nStackExchange\\n300\\n0.717\\n78.4\\n0.599\\n70.7\\n0.628\\n73.4\\n-5.0\\nWikihow QA\\n300\\n0.93\\n82.9\\n0.937\\n84.4\\n0.950\\n88.2\\n+5.3\\nResearchy Questions\\n1k\\nN/A\\n82.7\\nN/A\\n84.6\\nN/A\\n88.3\\n+5.6\\nTable 5: Comparison of various question decomposition techniques to prompt GPT-4 as an answer module. The top\\nhalf of the table is short-form questions, the bottom half is longform questions.\\nanother example is shown in the bottom of Table 2.\\nAlthough we don’t have a good way to quantify\\nthe prevalence of Pivotal Facts, we believe that\\nResearchy Questions is the best dataset to study\\nthe dynamics of how LLM Agents search for, and\\nrespond to, these uknown unknowns.\\n4\\nAgreement with User Search Behavior\\nMore complicated questions ought to require more\\neffort to answer (Kelly et al., 2015). We can approx-\\nimate the amount of effort users expend in terms of\\nbehavior signals like clicks and session length.\\nIn Table 4 (Left) we show aggregate click statis-\\ntics for each subset of queries. The results show\\nthat Researchy Questions , which are both non-\\nfactoid and decompositional, lead to more in-depth\\nconsumption (clicks and sat-clicks) of more di-\\nverse information (unique urls), agreeing with prior\\nwork (Hassan et al., 2014).\\nIn Table 4 (Right) we show behavioral signals\\nexhibited by users at the session level rather than\\nindividual click level. For instance, if a QnA-type\\nquery appeared at any time in any session in the\\ndate range, the whole session is included in the\\n“QnA Sessions” row. The results clearly show users\\nwere twice as engaged answering non-factoid ques-\\ntions than sessions devoted to factoid-y intents, and\\nsix times longer than the average session overall.\\n5\\nEvaluating Answer Techniques to\\nResearchy Questions\\nSince there is no one “correct” answer to a Re-\\nsearchy Question, we contend that they ought be\\nevaluated in a relative, side-by-side fashion a la\\nAlpaca-Eval syle (Li et al., 2023), with e.g. the\\n“closed-book” answer as the reference.\\nSince Researchy Questions are intended to be\\nanswered by decomposing them into sub-questions,\\nwe evaluate two decompositional question answer-\\ning answer techniques – chain-of-thought decom-\\nposition, and factored decomposition – against the\\ndirect answering baseline. Factored decomposition\\nmakes a separate call to an LLM for each sub-\\nquestion independently, and then a final “recom-\\nposition” call to synthesize the main answer (Rad-\\nhakrishnan et al., 2023).\\nTable 5 shows the results of a side-by-side auto-\\neval of the three answering techniques across a\\nrange of datasets. The LLM providing the answers\\nwas GPT-4, and the LLM-as-a-judge was also GPT-\\n4, prompted separately using the template in Fig-\\nure 8. Since the gold correct answers exist for many\\nof the datasets we consider, the judge was asked to\\nascertain “Accuracy” as a binary score of whether\\nthe candidate answer was consistent with the gold.\\nThe “Score” is on a scale of 1-100 indicating the\\noverall quality. The top half of Table 5 corresponds\\nto short-answer datasets (where Accuracy is more\\nimportant), while the bottom half is longer answer\\nquestions where the overall Score is better suited\\nto evaluate. Out of the long-form answer datasets,\\nResearchy Questions benefits the most from de-\\ncomposition techniques.\\nWe draw several conclusions from Table 5.\\nFirstly, we largely confirm the results of (Rad-\\nhakrishnan et al., 2023) that decompositional tech-\\nniques improve the accuracy on short-form multi-\\nhop datasets over zero-shot direct answering. Sec-\\nondly, factored decomposition leads to the highest\\nscores for long-form questions, particularly ques-\\ntions involving reasoning about complex processes\\nlike Wikihow and Researchy Questions . On the\\nother hand, chain-of-thought decomposition may\\nbe better suited to deduce correct answers to more\\nlogic-intensive questions. We believe the results\\nfor Researchy Questions using factored decom-\\nposition would be much higher if we incorporated\\nretrieved information.\\n6\\nRelated Work\\n6.1\\nHuman Behavior in Search Sessions\\nThere has been some fundamental research into\\nunderstanding user behavior in search sessions,\\nfrom user studies (Kelly et al., 2015) to large scale\\nclick log evaluations (Hassan et al., 2014). The\\nlatter attempt to ascertain signals that could in-\\ndicate whether a user is “struggling” or “explor-\\ning” in their search session; we use many of the\\nsame signals. Namely, they conclude that “explor-\\ning” sessions contain more clicks because users\\nwant to locate information for multiple facets of\\na topic. We concur with their results, e.g. non-\\nfactoid questions involve more clicks than fac-\\ntoid ones. Similarly, the definition of a “complex\\nsearch task” is almost congruent to that of a Re-\\nsearchy Question in its information seeking behav-\\niors Aula and Russell. Other studies try to iden-\\ntify complex search tasks and offer recommended\\nsub-tasks (Hassan Awadallah et al., 2014; Zhang\\net al., 2021). Much of the intuition on how to fil-\\nter Researchy Questions stems from these and\\nsimilar studies on user search behavior.\\n6.2\\nIterative Retrieval-Augmented Generation\\nMany\\npapers\\ndiscuss\\nvariants\\nof\\nchain-of-\\nthought (Wei et al., 2023) adapted to decompos-\\ning multi-faceted questions such as query refine-\\nment prompts (Amplayo et al., 2022), Decomposed\\nPrompting (Khot et al., 2023) and ReAct (Yao\\net al., 2023b). A step further is to marry a gen-\\nerative LLM with a vector-based retrieval sys-\\ntem, e.g. (Ren et al., 2023; Xiong et al., 2020;\\nKarpukhin et al., 2020; Izacard and Grave, 2021).\\nSuch methods are instantiated in many flavors:\\nSelf-Ask (Press et al., 2023) was one of the first\\nchain-of-thought-like decomposed prompting tech-\\nniques to integrate a search engine, forcing an LLM\\nto iteratively ask and subquestions. IRCoT inter-\\nleaves retrieval with chain-of-thought such that\\nwhat to retrieve next depends on what was previ-\\nously retrieved (Trivedi et al., 2023) Iter-RetGen it-\\neratively retrieves and generates candidate answers\\nto use as inputs to the next phase of retrieval (Shao\\net al., 2023), while Beam Retrieval maintains run-\\nning hypotheses of the relevant passages at every\\nstep (Zhang et al., 2023).\\n6.3\\nAgentic Question Answering\\nThere are several “agentic” frameworks to facilitate\\nthe dynamic interaction between tools e.g. retrieval\\nsystems and LLMs-as-agents which are adept at\\niterative retrieval-augmented QA tasks. One is\\nDemonstrate-Search-Predict (Khattab et al., 2023),\\nanother is AutoGen (Wu et al., 2023b). Some exist-\\ning Web-based Agents include WebGPT (Nakano\\net al., 2022) which imitate how users surf the web\\nand ask queries, others like WebAgent (Gur et al.,\\n2023) interact with the web programmatically by\\nunderstanding the raw HTML.\\nThere are also several consumer-facing products\\nfor agentic search assistants such as Bing Chat 3,\\nYouPro 4 (Research Mode) and SciPhi 5. All of\\nthese systems clearly decompose queries into sub-\\nquestions and then retrieve/crawl the necessary\\npages to synthesize the final outputs. An exam-\\nple of each addressing a Researchy Question is\\nshown in Figures 9, 10, and 11, respectively.\\n7\\nConclusion\\nFor some time now, many QA datasets have been\\nsaturated, and many researchers have dismissed\\nsearch logs as sources of complex questions. We\\npresent Researchy Questions , a large dataset to\\nadvance the field of complex question answering\\nover multiple documents and perspectives, target-\\ning LLM-assisted web search agents. We detail\\nhow these complex queries are mined from search\\nlogs, and confirm they involve significantly more\\neffort than other types of search queries. We also\\nprovide some preliminary evidence that decompo-\\nsitional answering techniques yield stronger results\\non Researchy Questions than direct answering.\\nBy design, these questions don’t have a gold\\nanswer, so regrettably, it difficult to quantify the\\n“headroom” of existing models, but qualitatively\\n(e.g. Table 10) there seems to be substantial room\\nfor improvement. There also remains much work\\nsurrounding how to measure the quality of sub-\\nquestions, and studying how LLM agents find and\\ninteract with Pivotal Facts. We hope this dataset\\ncan help aid the development of new metrics and\\nunlock new experiences for web users.\\n3https://bing.com/chat\\n4https://you.com/search\\n5https://search.sciphi.ai/research\\nLimitations\\nOne of the primary limitations of this study is that\\nwhile we argue question decomposition is key to\\nsolving complex Researchy Questions , we don’t\\npropose a good way to measure the quality of a\\ncandidate set of sub-questions. Instead, we loosely\\nargue that good sub-questions ought to at least lead\\nto the same set of information found in the docu-\\nments users clicked on for the question. We admit\\nthese two things are not the same, but clicks are a\\npowerful signal. We try to make up for this gap by\\nreleasing the hierarchical question/query decom-\\nposition that GPT-4 gives closed-book, but we do\\nnot evaluate whether that decomposition does in-\\ndeed lead to the same set of gold documents in a\\nreal retrieval system e.g. built from the entirety of\\nClueWeb22.\\nWe also admit that it would potentially be better\\nto produce the question decomposition by “work-\\ning backwards” from the documents that were\\nclicked on – i.e. identify key information in the\\nclicked document for a researchy question and then\\nidentify what sub-question would lead to that in-\\nformation being retrieved. On the other hand, one\\ncould argue that the “forward” direction is what\\nneeds to be implemented in practice. While we\\ndo not address these points in this study, others in\\nthe academic community can explore these topics\\nmore with the dataset we release.\\nAnother limitation of this study is that Pivotal\\nFacts are purely an observational phenomenon that\\nwe have in no way quantified. Future researchers\\ncould create a LLM-prompt to count the number of\\nsuch statements in the clicked documents.\\nWe regret that this dataset is not multi-lingual.\\nThis is due to the large amount of uncertainty and\\ntrial-and-error in curating this dataset, which meant\\nfrequent manual inspection of data. We believe the\\nsame framework can be used to construct a multi-\\nlingual version of Researchy Questions .\\nEthics Statement\\nWe obtained approval to release this dataset after\\nthorough IRB proceedings to ensure compliance\\nwith privacy, safety, and legal guidelines.\\nWe would like to make a few notes: while it may\\nappear that we tried to remove queries that seemed\\n“controversial”, it is not our intention to act as moral\\nor political regulators to decide if user queries are\\nwell-meaning. At the scale of web search, one will\\nnotice that users ask a lot of questions for a lot of\\nreasons, and it is beyond the scope of our job to\\nspeculate what the motives are behind a query. It is\\nis our job to assess whether the act of attempting\\nto answer a question will lead to a reasonable risk\\nof harm. Also, it is not the case that “GPT-4 is our\\nmoral authority” on the topic of safety, it is merely\\none of many tools we employ in a stack to ensure\\nour internal requirements are being met.\\nReferences\\nReinald Kim Amplayo, Kellie Webster, Michael Collins,\\nDipanjan Das, and Shashi Narayan. 2022. Query\\nrefinement prompts for closed-book long-form ques-\\ntion answering.\\nAnne Aula and Daniel Russell. 2008. Complex and\\nexploratory web search.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir\\nRosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,\\nand Tong Wang. 2018. Ms marco: A human gener-\\nated machine reading comprehension dataset.\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\\nLiang. 2013. Semantic parsing on Freebase from\\nquestion-answer pairs. In Proceedings of the 2013\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 1533–1544, Seattle, Wash-\\nington, USA. Association for Computational Linguis-\\ntics.\\nValeriia Bolotova, Vladislav Blinov, Falk Scholer,\\nW. Bruce Croft, and Mark Sanderson. 2022. A non-\\nfactoid question-answering taxonomy. In Proceed-\\nings of the 45th International ACM SIGIR Confer-\\nence on Research and Development in Information\\nRetrieval, SIGIR ’22, page 1196–1207, New York,\\nNY, USA. Association for Computing Machinery.\\nValeriia Bolotova-Baranova, Vladislav Blinov, Sofya\\nFilippova, Falk Scholer, and Mark Sanderson. 2023.\\nWikiHowQA: A comprehensive benchmark for multi-\\ndocument non-factoid question answering. In Pro-\\nceedings of the 61st Annual Meeting of the Associa-\\ntion for Computational Linguistics (Volume 1: Long\\nPapers), pages 5291–5314, Toronto, Canada. Associ-\\nation for Computational Linguistics.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\\n2022. Improving language models by retrieving from\\ntrillions of tokens.\\nFan Bu, Xingwei Zhu, Yu Hao, and Xiaoyan Zhu. 2010.\\nFunction-based question classification for general\\nQA. In Proceedings of the 2010 Conference on Em-\\npirical Methods in Natural Language Processing,\\npages 1119–1128, Cambridge, MA. Association for\\nComputational Linguistics.\\nDaniel Fernando Campos, Tri Nguyen, Mir Rosenberg,\\nXia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\\nMajumder, Li Deng, and Bhaskar Mitra. 2016. Ms\\nmarco: A human generated machine reading compre-\\nhension dataset. ArXiv, abs/1611.09268.\\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong,\\nHong Wang, and William Yang Wang. 2020. Hy-\\nbridQA: A dataset of multi-hop question answering\\nover tabular and textual data. In Findings of the Asso-\\nciation for Computational Linguistics: EMNLP 2020,\\npages 1026–1036, Online. Association for Computa-\\ntional Linguistics.\\nElvis Dohmatob, Yunzhen Feng, Pu Yang, Francois\\nCharton, and Julia Kempe. 2024. A tale of tails:\\nModel collapse as a change of scaling laws.\\nMatthew Dunn, Levent Sagun, Mike Higgins, V. Ugur\\nGuney, Volkan Cirik, and Kyunghyun Cho. 2017.\\nSearchqa: A new q&a dataset augmented with con-\\ntext from a search engine.\\nBrian Everitt. 1974. Cluster analysis. Heinemann Edu-\\ncational [for] the Social Science Research Council.\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\\nier, Jason Weston, and Michael Auli. 2019. ELI5:\\nlong form question answering. In Proceedings of\\nthe 57th Conference of the Association for Compu-\\ntational Linguistics, ACL 2019, Florence, Italy, July\\n28- August 2, 2019, Volume 1: Long Papers, pages\\n3558–3567. Association for Computational Linguis-\\ntics.\\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\\ning, Travis Hoppe, Charles Foster, Jason Phang,\\nHorace He, Anish Thite, Noa Nabeshima, Shawn\\nPresser, and Connor Leahy. 2020.\\nThe pile: An\\n800gb dataset of diverse text for language modeling.\\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa\\nSafdari, Yutaka Matsuo, Douglas Eck, and Aleksan-\\ndra Faust. 2023. A real-world webagent with plan-\\nning, long context understanding, and program syn-\\nthesis.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training.\\nAhmed Hassan, Ryen W. White, Susan T. Dumais, and\\nYi-Min Wang. 2014. Struggling or exploring? dis-\\nambiguating long search sessions. In Proceedings\\nof the 7th ACM International Conference on Web\\nSearch and Data Mining, WSDM ’14, page 53–62,\\nNew York, NY, USA. Association for Computing\\nMachinery.\\nAhmed Hassan Awadallah, Ryen W. White, Patrick Pan-\\ntel, Susan T. Dumais, and Yi-Min Wang. 2014. Sup-\\nporting complex search tasks. In Proceedings of the\\n23rd ACM International Conference on Conference\\non Information and Knowledge Management, CIKM\\n’14, page 829–838, New York, NY, USA. Association\\nfor Computing Machinery.\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open do-\\nmain question answering. In Proceedings of the 16th\\nConference of the European Chapter of the Associ-\\nation for Computational Linguistics: Main Volume,\\npages 874–880, Online. Association for Computa-\\ntional Linguistics.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine\\nRoux, Arthur Mensch, Blanche Savary, Chris\\nBamford, Devendra Singh Chaplot, Diego de las\\nCasas, Emma Bou Hanna, Florian Bressand, Gi-\\nanna Lengyel, Guillaume Bour, Guillaume Lam-\\nple, Lélio Renard Lavaud, Lucile Saulnier, Marie-\\nAnne Lachaux, Pierre Stock, Sandeep Subramanian,\\nSophia Yang, Szymon Antoniak, Teven Le Scao,\\nThéophile Gervet, Thibaut Lavril, Thomas Wang,\\nTimothée Lacroix, and William El Sayed. 2024. Mix-\\ntral of experts.\\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\\nBillion-scale similarity search with GPUs.\\nIEEE\\nTransactions on Big Data, 7(3):535–547.\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. triviaqa: A Large Scale Distantly\\nSupervised Challenge Dataset for Reading Compre-\\nhension. arXiv e-prints, page arXiv:1705.03551.\\nDaniel Kahneman. 2011. Thinking, Fast and Slow. Far-\\nrar, Straus and Giroux, New York.\\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen tau Yih. 2020. Dense passage retrieval for open-\\ndomain question answering.\\nDiane Kelly, Jaime Arguello, Ashlee Edwards, and Wan-\\nching Wu. 2015. Development and evaluation of\\nsearch tasks for iir experiments using a cognitive\\ncomplexity framework. In Proceedings of the 2015\\nInternational Conference on The Theory of Informa-\\ntion Retrieval, ICTIR ’15, page 101–110, New York,\\nNY, USA. Association for Computing Machinery.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\\nDavid Hall, Percy Liang, Christopher Potts, and\\nMatei Zaharia. 2023. Demonstrate-search-predict:\\nComposing retrieval and language models for\\nknowledge-intensive nlp.\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\\nKyle Richardson, Peter Clark, and Ashish Sabharwal.\\n2023. Decomposed prompting: A modular approach\\nfor solving complex tasks.\\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\\nHurdles to progress in long-form question answering.\\nSayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha,\\nand Eugene Ie. 2020. Aquamuse: Automatically\\ngenerating datasets for query-based multi-document\\nsummarization.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: a benchmark for question answering\\nresearch. Transactions of the Association of Compu-\\ntational Linguistics.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-\\ntäschel, Sebastian Riedel, and Douwe Kiela. 2021.\\nRetrieval-augmented generation for knowledge-\\nintensive nlp tasks.\\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,\\nIshaan Gulrajani, Carlos Guestrin, Percy Liang, and\\nTatsunori B. Hashimoto. 2023. Alpacaeval: An au-\\ntomatic evaluator of instruction-following models.\\nhttps://github.com/tatsu-lab/alpaca_eval.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\\njape, Michele Bevilacqua, Fabio Petroni, and Percy\\nLiang. 2023a. Lost in the middle: How language\\nmodels use long contexts.\\nNelson F. Liu, Tianyi Zhang, and Percy Liang. 2023b.\\nEvaluating verifiability in generative search engines.\\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu\\nLei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen\\nMen, Kejuan Yang, Shudan Zhang, Xiang Deng, Ao-\\nhan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng\\nShen, Tianjun Zhang, Yu Su, Huan Sun, Minlie\\nHuang, Yuxiao Dong, and Jie Tang. 2023c. Agent-\\nbench: Evaluating llms as agents.\\nGrégoire Mialon, Clémentine Fourrier, Craig Swift,\\nThomas Wolf, Yann LeCun, and Thomas Scialom.\\n2023. Gaia: a benchmark for general ai assistants.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\\nKrueger, Kevin Button, Matthew Knight, Benjamin\\nChess, and John Schulman. 2022. Webgpt: Browser-\\nassisted question-answering with human feedback.\\nOpenAI, :, Josh Achiam, Steven Adler, Sandhini Agar-\\nwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\\ning Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,\\nJake Berdine, Gabriel Bernadett-Shapiro, Christo-\\npher Berner, Lenny Bogdonoff, Oleg Boiko, Made-\\nlaine Boyd, Anna-Luisa Brakman, Greg Brockman,\\nTim Brooks, Miles Brundage, Kevin Button, Trevor\\nCai, Rosie Campbell, Andrew Cann, Brittany Carey,\\nChelsea Carlson, Rory Carmichael, Brooke Chan,\\nChe Chang, Fotis Chantzis, Derek Chen, Sully Chen,\\nRuby Chen, Jason Chen, Mark Chen, Ben Chess,\\nChester Cho, Casey Chu, Hyung Won Chung, Dave\\nCummings, Jeremiah Currier, Yunxing Dai, Cory\\nDecareaux, Thomas Degry, Noah Deutsch, Damien\\nDeville, Arka Dhar, David Dohan, Steve Dowl-\\ning, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\\nTyna Eloundou, David Farhi, Liam Fedus, Niko\\nFelix, Simón Posada Fishman, Juston Forte, Is-\\nabella Fulford, Leo Gao, Elie Georges, Christian\\nGibson, Vik Goel, Tarun Gogineni, Gabriel Goh,\\nRapha Gontijo-Lopes, Jonathan Gordon, Morgan\\nGrafstein, Scott Gray, Ryan Greene, Joshua Gross,\\nShixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse\\nHan, Jeff Harris, Yuchen He, Mike Heaton, Jo-\\nhannes Heidecke, Chris Hesse, Alan Hickey, Wade\\nHickey, Peter Hoeschele, Brandon Houghton, Kenny\\nHsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger\\nJiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie\\nJonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,\\nAli Kamali, Ingmar Kanitscheider, Nitish Shirish\\nKeskar, Tabarak Khan, Logan Kilpatrick, Jong Wook\\nKim, Christina Kim, Yongjik Kim, Hendrik Kirch-\\nner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\\nŁukasz Kondraciuk, Andrew Kondrich, Aris Kon-\\nstantinidis, Kyle Kosic, Gretchen Krueger, Vishal\\nKuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan\\nLeike, Jade Leung, Daniel Levy, Chak Ming Li,\\nRachel Lim, Molly Lin, Stephanie Lin, Mateusz\\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,\\nAnna Makanju, Kim Malfacini, Sam Manning, Todor\\nMarkov, Yaniv Markovski, Bianca Martin, Katie\\nMayer, Andrew Mayne, Bob McGrew, Scott Mayer\\nMcKinney, Christine McLeavey, Paul McMillan,\\nJake McNeil, David Medina, Aalok Mehta, Jacob\\nMenick, Luke Metz, Andrey Mishchenko, Pamela\\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel\\nMossing, Tong Mu, Mira Murati, Oleg Murk, David\\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,\\nArvind Neelakantan, Richard Ngo, Hyeonwoo Noh,\\nLong Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex\\nPaino, Joe Palermo, Ashley Pantuliano, Giambat-\\ntista Parascandolo, Joel Parish, Emy Parparita, Alex\\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-\\nman, Filipe de Avila Belbute Peres, Michael Petrov,\\nHenrique Ponde de Oliveira Pinto, Michael, Poko-\\nrny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-\\nell, Alethea Power, Boris Power, Elizabeth Proehl,\\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\\nCameron Raymond, Francis Real, Kendra Rimbach,\\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ry-\\nder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\\nGirish Sastry, Heather Schmidt, David Schnurr, John\\nSchulman, Daniel Selsam, Kyla Sheppard, Toki\\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav\\nShyam, Szymon Sidor, Eric Sigler, Maddie Simens,\\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin\\nSokolowsky, Yang Song, Natalie Staudacher, Fe-\\nlipe Petroski Such, Natalie Summers, Ilya Sutskever,\\nJie Tang, Nikolas Tezak, Madeleine Thompson, Phil\\nTillet, Amin Tootoonchian, Elizabeth Tseng, Pre-\\nston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\\nlipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\\nChelsea Voss, Carroll Wainwright, Justin Jay Wang,\\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei,\\nCJ Weinmann, Akila Welihinda, Peter Welinder, Ji-\\nayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\\nClemens Winter, Samuel Wolrich, Hannah Wong,\\nLauren Workman, Sherwin Wu, Jeff Wu, Michael\\nWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-\\ning Yuan, Wojciech Zaremba, Rowan Zellers, Chong\\nZhang, Marvin Zhang, Shengjia Zhao, Tianhao\\nZheng, Juntang Zhuang, William Zhuk, and Barret\\nZoph. 2023. Gpt-4 technical report.\\nArnold Overwijk, Chenyan Xiong, Xiao Liu, Cameron\\nVandenBerg, and Jamie Callan. 2022. Clueweb22:\\n10 billion web documents with visual and semantic\\ninformation.\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\\nley, and Jianfeng Gao. 2023. Instruction tuning with\\ngpt-4. arXiv preprint arXiv:2304.03277.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A. Smith, and Mike Lewis. 2023. Measuring\\nand narrowing the compositionality gap in language\\nmodels.\\nAnsh Radhakrishnan, Karina Nguyen, Anna Chen,\\nCarol Chen, Carson Denison, Danny Hernandez,\\nEsin Durmus, Evan Hubinger, Jackson Kernion,\\nKamil˙e Lukoši¯ut˙e, Newton Cheng, Nicholas Joseph,\\nNicholas Schiefer, Oliver Rausch, Sam McCandlish,\\nSheer El Showk, Tamera Lanham, Tim Maxwell,\\nVenkatesa Chandrasekaran, Zac Hatfield-Dodds,\\nJared Kaplan, Jan Brauner, Samuel R. Bowman, and\\nEthan Perez. 2023.\\nQuestion decomposition im-\\nproves the faithfulness of model-generated reasoning.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2023. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer.\\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\\nQiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong\\nWen. 2023. Rocketqav2: A joint training method for\\ndense passage retrieval and passage re-ranking.\\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie\\nHuang, Nan Duan, and Weizhu Chen. 2023. En-\\nhancing retrieval-augmented large language models\\nwith iterative retrieval-generation synergy.\\nC. T. Stayton. 2015. What does convergent evolution\\nmean? the interpretation of convergence and its impli-\\ncations in the search for limits to evolution. Interface\\nFocus, 5(6):20150039.\\nNassim Nicholas Taleb. 2008. The Black Swan. Pen-\\nguin Books, Harlow, England.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2022.\\nMuSiQue: Multi-\\nhop questions via single-hop question composition.\\nTransactions of the Association for Computational\\nLinguistics.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2023. Interleaving retrieval\\nwith chain-of-thought reasoning for knowledge-\\nintensive multi-step questions.\\nHouse Committee on Science United States Congress,\\nSubcommittee on Space Science Technology, and Ap-\\nplications. 1981. NASA Program Management and\\nProcurement Procedures and Practices: Hearings\\nBefore the Subcommittee on Space Science and Ap-\\nplications of the Committee on Science and Technol-\\nogy, U.S. House of Representatives, Ninety-seventh\\nCongress, First Session. U.S. Government Printing\\nOffice, Washington, D.C.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\\nDenny Zhou. 2023. Chain-of-thought prompting elic-\\nits reasoning in large language models.\\nJiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng\\nWan, and Hong Lin. 2023a. Ai-generated content\\n(aigc): A survey.\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\\nBeibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,\\nShaokun Zhang, Jiale Liu, Ahmed Hassan Awadal-\\nlah, Ryen W White, Doug Burger, and Chi Wang.\\n2023b. Autogen: Enabling next-gen llm applications\\nvia multi-agent conversation.\\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\\nOverwijk. 2020. Approximate nearest neighbor neg-\\native contrastive learning for dense text retrieval.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\\ngio, William W. Cohen, Ruslan Salakhutdinov, and\\nChristopher D. Manning. 2018. Hotpotqa: A dataset\\nfor diverse, explainable multi-hop question answer-\\ning.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L. Griffiths,\\nYuan Cao,\\nand Karthik\\nNarasimhan. 2023a. Tree of thoughts: Deliberate\\nproblem solving with large language models.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2023b.\\nReact: Synergizing reasoning and acting in language\\nmodels.\\nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho,\\nXian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason\\nWeston. 2024. Self-rewarding language models.\\nJiahao Zhang, Haiyang Zhang, Dongmei Zhang, Yong\\nLiu, and Shen Huang. 2023. Beam retrieval: General\\nend-to-end retrieval for multi-hop question answer-\\ning.\\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2016.\\nCharacter-level convolutional networks for text clas-\\nsification.\\nYi Zhang, Sujay Kumar Jauhar, Julia Kiseleva, Ryen\\nWhite, and Dan Roth. 2021. Learning to decom-\\npose and organize complex tasks. In Proceedings of\\nthe 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, pages 2726–2735,\\nOnline. Association for Computational Linguistics.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\\nJoseph E. Gonzalez, and Ion Stoica. 2023a. Judging\\nllm-as-a-judge with mt-bench and chatbot arena.\\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.\\n2023b. Why does chatgpt fall short in providing\\ntruthful answers?\\nKun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen,\\nWayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong\\nWen, and Jiawei Han. 2023. Don’t make your llm an\\nevaluation benchmark cheater.\\nFigure 2: (Right) Histogram of number of documents clicked per question for Researchy Questions which is\\nmuch higher than for general web search queries. (Left) number of queries associated with each document. The fact\\nthat not very many queries are associated with each document validates the effectiveness of our query-deduplication\\nprocedure.\\nA\\nGPT-4-based Filtering Details\\nWe had GPT-4 label certain attributes of each Researchy Question using the prompt given in Figure 7.\\nParticularly, we focus on removing questions that were too incomplete or underspecified to meaningfully\\nanswer (shown in Table 7), or those that are too presumptuous (shown in Table 8). Incomplete queries\\ncould arise when users ask queries that e.g. refer to previous queries or subjects in their session.\\nA.1\\nAdditional Safety Filtering\\nIn addition to the official offensiveness/adult query classifiers used in our commericial search engine, we\\nalso had GPT-4 label on a scale of 1-10 “to what extent the question could be interpreted as being harmful\\n(physically or psychologically to oneself, others, or animals), offensive, overly biased, sexually explicit, or\\notherwise inappropriate for e.g. someone of the age of 12 to be exposed to.”. We found roughly 2% of\\nqueries had an unsatisfactory harmfulness score and hence were removed from the released dataset.\\nTable 9 shows some examples of those removed. Real people asked these questions: we acknowledge\\nthat some of these may be motivated by genuine information-seeking intent, notwithstanding, we believe\\nthere is too high a risk that the act of answering these questions may lead to harm.\\nFigure 3: (Left) Non-factoid scores of the 15.7M QnA Queries. The roughly 1M queries whose score exceeded the\\nthreshold +0.75 were sent to the Decompositional classifier. Note that because this was a binary classifier, 89%\\nof the non-factoid scores were less than -0.75, which is cut off from the left-hand histogram to make it easier to\\nvisualize. (Right) The Decompositional classifier’s scores of the roughly 1M Non-factoid queries. Around 146k\\nqueries exceeding the 0.6 threshold line resulted are considered both Non-factoid and Decompositional, and were\\nthen de-duplicated to arrive at the final Researchy Questions dataset of around 100k.\\nQuestion\\nFactoid\\nDecompositional (↓)\\nWhy is it illegal to destroy money\\n1.02\\n0.59\\nWhich law firms offer the best opportunities for international work?\\n1.07\\n0.58\\nHow should a grad student communicate with faculty\\n0.80\\n0.56\\nHow does a short story differ from a novel\\n1.07\\n0.54\\nWhy is Beowulf an important literary work\\n1.10\\n0.51\\nWhy are e scooters illegal\\n0.90\\n0.50\\nWhy did the declaration of independence start\\n1.12\\n0.49\\nHow did marsupials evolve\\n1.02\\n0.48\\nIs turbulence dangerous\\n0.97\\n0.47\\nWhy do people cut down the Amazon rainforest\\n1.09\\n0.45\\nWhy are flags at half mast in Indiana today\\n1.06\\n0.45\\nWhy the houses in Rajasthan have thick walls and flat roofs?\\n1.06\\n0.43\\nHow do forensic pathologists determine cause and manner of death?\\n1.02\\n0.43\\nWhat causes changes in the moon’s appearance\\n1.10\\n0.41\\nHow did Edgar Allan Poe start his writing career\\n0.97\\n0.40\\nHow common was vasectomy in the 1950’s\\n0.90\\n0.40\\nWhy are behavioral questions important\\n0.79\\n0.38\\nIs pepper spray harmful\\n0.93\\n0.37\\nHow do fertilizers increase productivity\\n1.05\\n0.35\\nHow do we get matter and energy from food\\n1.11\\n0.32\\nWhat type of competition occurs when only a few firms dominate a market?\\n1.08\\n0.31\\nWhy can’t waves travel through a vacuum?\\n1.08\\n0.30\\nWhat caused the Dixie fire\\n0.84\\n0.29\\nWhy camels live in desert\\n1.02\\n0.29\\nHow was COVID-19 discovered\\n0.92\\n0.28\\nHow are elements organized into groups\\n0.96\\n0.27\\nWhy did Montezuma give the Spanish gold?\\n1.01\\n0.27\\nDifferences between debt and equity financing\\n0.98\\n0.27\\nHow are fossils older than 60,000 years dated?\\n1.08\\n0.17\\nWhat happens when a data collision occurs on an Ethernet bus?\\n1.03\\n0.16\\nHow is gold formed in the earth’s crust\\n0.98\\n0.12\\nTable 6: Examples of QnA Queries that were non-factoid (had a non-factoid classifier score above 0.75), but\\nwere not decompositional (had decompositional classifier score less than 0.6). Further down this list, more of the\\nexamples have a single correct answer (even if it is somewhat long-form), but there is obviously a lot of gray area,\\nhighlighting the challenges of trying to filter large amounts of queries by simple thresholds.\\n### For the following (Question | score | reason) triples, the score indicates how \"good\" of a non-\\nfactoid question they are in the sense that they can lead to interesting and in-depth analysis.\\n### Definition: A good non-factoid question is specific, with potential to amount to a good research\\nreport with a clear and refutable thesis, supported by evidence and analysis.\\n### Characteristic formats of good non-factoid questions (not exhaustive):\\n• Good non-factoid questions will often talk about the relationship between two things, e.g.\\n\"Compare and contrast X and Y\", \"How/why does X affect/impact Y?\", \"Why X is significant to\\nY\", or \"What role does X play in Y?\", or \"to what extent does X lead to Y?\", etc.\\n• A good non-factoid question can also ask \"Why does X happen\", \"What factors play a role in\\nX?\", \"How is X significant\" or \"What is the cause of X\", but it should be specific about what\\nkind of analysis is expected.\\n• Other forms of good non-factoid questions can ask about the pros/cons, benefits/detriments of\\nsomething, or compare/contrast two things, etc.\\n### Instructions: Rate each question on a scale of 0-10, where 0 is a factoid question and 10 is an\\nexcellent non-factoid question and then provide a brief reason for your rating\\nQ: how tall is abraham lincoln | 0 | factoid\\nQ: can i change the weather | 2 | personal question\\nQ: was the civil war fought over slavery | 5 | fair, but could more directly ask about other important\\nfacets of the causes of the civil war and their role in the conflict\\nQ: to what extent was the civil war fought over slavery | 8 | good, will lead to in-depth analysis on the\\ncauses of the civil war\\nQ: what impact do human activities have on the weather | 10 | excellent, many in-depth reports written\\nto answer this question\\nQ: should LA invest more in railway or highway infrastructure for public transport | 9 | great\\nQ: what is an example of blackbody radiation? | 0 | asking for an example\\nQ: could not determine type for | 0 | not a question\\nQ: what typically signals the end of the olympic games | 2 | factoid, olympic closing ceremony can be\\nlooked up easily\\nQ: Why were Navajo code talkers used during WW2? | 7 | good, could lead to analysis of how culture\\nand language can be used in warfare\\nQ: When does protein folding begin? | 1 | has a single, known correct answer\\nQ: what is the cost and necessary materials to build a refinery | 5 | fair, asks about a complex process\\nbut will not likely elicit analysis\\nQ: What is the Navavidha Bhakti? | 0 | asking for a definition\\nQ: why is technological change bad? | 5 | fair, but could be more specific\\nQ: analyze how technological changes have historically impacted cultures | 10 | excellent, very specific\\nQ: who owns phone number 280-626-1435 | 0 | personally identifiable information\\nQ: What are the main differences between regulations of the NFL and the CFL? | 4 | has potential for\\nin-depth analysis but doesn’t explicitly ask for it\\nQ: Why do planes using rivets & not welded construction? | 7 | good, will require in-depth analysis\\non aerospace technology\\nQ: How did the Catholic Pope manage to become more powerful than Kings in old Europe? | 9 | much\\npotential for historical analysis\\nQ: interesting facts about korea | 0 | not specific\\nQ: {Question} |\\nFigure 4: Prompt given to text-davinci-003 to collect labels of whether a question is non-factoid. The current\\nquestion is substituted at the end. Labels on 1-10 were binarized based to train the non-factoid classifier.\\nQuestion: {Question}\\n### Instructions: how clearly does the above question require sub-questions in order to answer it?\\nThink silently about how complex or multi-facted the question is. Imagine you are a person trying to\\nanswer the question using a search engine like Google. Would the person likely have to issue multiple\\nqueries to arrive at a comprehensive answer? Would they need to exert more effort to fully understand\\nthe nuances behind the question, or different perspectives surrounding it? etc. Or, is the person likely\\nto find the best answer with a single search with low effort? Give your rating on 1-100 between\\n<score> and </score> tags, where 1 means the question is trivial or common knowledge, while 100\\nmeans it likely involves effort to break the question down into many sub-questions or facets. <score>\\nFigure 5: Prompt given to gpt-35-turbo to collect labels of how appropriate a question is for “decomposition” into\\nsub-questions. These labels were used to train the Decompositional classifier.\\nhow can climate change\\nwhy did the market crash\\nare perspectives that change when presented with\\nfresh experiences\\nhow did the war for independence end\\nin what ways did native peoples transform\\nhow dangeous can a persona affect a person per-\\nsonality\\nhow would the law treat the issues confronting a\\ndiverse family?\\nhow did president lyndon b. johnson respond to\\nthis event?\\ncompare and contrast social contract\\nwhat is the difference in how the government re-\\nacted in these two situations?\\nhow were others influenced by miyamoto\\nexplain why summers were difficult for europeans.\\nwhat effect did the recent looting have on emigra-\\ntion\\nwhat is the main reason why some teams never\\nreach stage 4\\nTable 7: Examples of incomplete or ambiguous queries a.k.a too underspecified to attempt to answer meaningfully\\n(roughly 3% of queries in the last stage of filtering) that were removed from the dataset by additional GPT-4\\nfiltering.\\nhow college became a ruthless competition di-\\nvorced from learning\\nwhy is airline customer service so bad\\nwhy are police departments afraid of change\\nwhy immigration is bad for the us\\nwhy did chinese families not desire to have female\\nchildren?\\nwhy do games make people socially disconnected\\nfrom people\\nwhy are catholics democrats\\nwhy junior doctors are treated so badly\\nhow casinos have ruined towns\\nwhy should zoos be shut down\\nhow is the government controlling us\\nwhy rehabilitation does not work in america\\nwhy veganism is bad for animals\\nwhy most clinical research is not useful\\nTable 8: Examples of “presumptious” queries we removed from the dataset (roughly 2% of queries in the last stage\\nof filtering), roughly defined as those where the question is slanted or biased in some way. Note, not all assumptive\\nqueries were harmful.\\nFigure 6: We had GPT-4 label all 102k Researchy question – and a uniform sample of 2k Natural Questions –\\nalong 8 dimensions using the prompt in Figure 7. Each dimension was on a scale of 1-10; we plot their normalized\\ndensities. For Researchy Questions, these are the scores of the final 96k released questions after final filtering\\ndescribed in Section 2.5. Lower scores are better for Ambiguity, Incompleteness, Assumptiveness and Harmfulness.\\nWe intended Researchy Questions to have higher scores for Knowledge-intensiveness, Reasoning-intensiveness and\\nMulti-facetedness.\\nGiven the question: {Question}\\nInstructions:\\nPlease output a python dictionary with fields scoring the question on the fol-\\nlowing criteria:\\n1. \"ambiguous\" : Int 0-10 to what extent is the intent of the question ambiguous (has more than\\none interpretation); 0 means no major ambiguity. Not to be confused with subjectiveness or\\nincompleteness.\\n2. \"incompleteness\" : Int 0-10 indicating how difficult it is to determine the intent of the question,\\nwhether it is missing crucial context or details that ought to be specified in order to answer\\nthe question; 0 means the question is answerable and self-contained, 10 means the question is\\nun-answerable because it is incomplete or under-specified.\\n3. \"assumptive\" : Int 0-10 the degree to which the question has built-in assumptions or biases (that\\nare not offensive, which is point 8 below); 0 means no notable or unreasonable assumptions.\\n4. \"multi-faceted\" : Int 0-10 the degree to which the question has multiple facets or perspectives\\nthat need to be considered in order to answer it; 0 means the question is straightforward and has\\na single, undisputed answer.\\n5. \"knowledge-intensive\" : Int 0-10 the degree to which the question would require specialized\\nknowledge (like textbooks, scholarly articles, etc) to provide a thorough and grounded answer; 0\\nmeans the answer is common knowledge or can be looked up instantly in common references,\\n10 means the questions probably entails a lot of work to find and analyze specialized knowledge.\\n6. \"subjective\" : Int 0-10 the degree to which the question is subjective, meaning an answer(s)\\nexist, but there is no agreed-upon way to determine which one is better; 0 means the question is\\nlargely objective i.e. the overwhelming majority of people would agree on the answer if they\\nknew it.\\n7. \"reasoning-intensive\" : Int 0-10 the degree to which the question requires reasoning to syn-\\nthesize an answer; 0 means the question can be answered trivially e.g. by looking up a fact,\\nreferencing an encyclopedia or database, or using a calculator (once).\\n8. \"harmful\" : Int 0-10 to what extent the question could be interpreted as being harmful\\n(physically or psychologically to oneself, others, or animals), offensive, overly biased, sexually\\nexplicit, or otherwise inappropriate for e.g. someone of the age of 12 to be exposed to.\\nNote that the above criteria are not mutually exclusive, e.g. a question can be both subjective and\\nknowledge-intensive, for example \"is capitalism better than socialism\" would be both. Make sure to\\noutput only the valid python dictionary without comments or other extraneous output.\\nFigure 7: Prompt given to GPT-4 to classify attributes of each of the Researchy Questions. The output labels were\\nused to further filter the data for release.\\nIs it fair to blame younger people for covid spike\\nHow bad is illegal immigration\\nWhy do people pretend to be depressed\\nWhy veganism is bad for animals\\nwhat factors affect when deciding to terminate a\\ndown sydrome child\\nWhy is the business of sex so popular and lucra-\\ntive?\\nhow people make money illegally\\nWas the holocaust real?\\nWhy was gender queer banned\\nWhich countries persecute christians\\nWhy do so many women get abortions\\nWas the holocaust worse than slavery\\nCould a small nuclear war reverse global warming\\nHow do holocaust deniers try to gain followers?\\nIs china a threat to world peace\\nWhy christians believe conspiracy theories\\nWhy do so many incompetent men become leaders\\nWhy did africans sell other africans into slavery\\nWhy is bribery being used to force people to take\\nvaccine\\nWhy were many whites scared away from support-\\ning civil rights\\nWhat makes a church a cult\\nWhy did poor southerners fight for slavery\\nAre minorities more likely to commit crime\\nWhy isn’t christianity considered a cult\\nWhy corporal punishment should be legal\\nPros and cons of sexism\\nShould we stratify people based on intelligence or\\nother specific qualities?\\nWhy are we challenged by the arrival of migrants\\nin our country?\\nWhy are we protecting the unvaccinated\\nIs the death penalty racially motivated\\nWhy is bayonetta so sexualized\\nWhy is racial profiling important in society\\nDoes islam promote violence\\nIs affirmative action racism\\nWho is behind mass immigration\\nWhy is the vaccine killing people\\nWhy is the west so intent in bringing in so many\\nillegal migrants\\nWhy might the artist have thought reconstruction\\nwas “worse than slavery” for black southerners?\\nTable 9: Here we show some examples of the roughly 2% of queries that were removed from the dataset by\\nadditional GPT-4 harmfulness filtering.\\nQuestion: {Question}\\n#####################\\nReference Answer (assumed to be true): {ReferenceAnswer}\\n{CandidateAnswers}\\n#####################\\nKeep in mind the following Guidelines when evaluating the Candidate Answers:\\nGuidelines:\\n• Usefulness: A useful answer adds value by providing in-depth analysis, actionable steps, and relevant information\\nthat helps users make informed decisions, complete tasks, and understand complex problems in a simpler way.\\nIt also considers real-world scenarios, cause and effect relationships, and historical context to enhance the user’s\\nunderstanding.\\n• Relevance: A relevant answer directly addresses the question’s intent in a sensible and specific manner, without\\nbeing overly broad, redundant, or distracting. It should be clear, on-topic, and provide helpful information tailored to\\nthe question asked.\\n• Truthfulness in an answer means providing accurate and verifiable information, without making false claims, unveri-\\nfiable statements, or promoting misleading information. It should agree with the Reference Answer. Contradicting\\nthe Reference Answer *must* result in lower score.\\n• Groundedness in an answer means providing information supported by reliable sources and following a logical\\nchain of reasoning, instilling confidence in its accuracy. The answer should be based on credible evidence, address\\nuncertainties, and avoid relying on personal opinions, rumors, or hearsay.\\n• Thoroughness involves considering the full scope of a question, addressing multiple perspectives, scenarios, and\\noutcomes, and ensuring all important aspects are covered without omission. It requires analyzing pros and cons,\\nempathizing with various stakeholders, and addressing different interpretations or facets of a question.\\n##################### Instructions: Above are {NumCandidates} Candidate Answers to the Question: {Question}.\\nPlease read them carefully along with the Guidelines for how to evaluate an answer’s quality. Then:\\n1. Briefly critique each Candidate Answer with respect to the Guidelines, formatted in *bullet points* between\\n\"<CritiqueX>\" and \"</CritiqueX>\" tags where X is the Candidate Answer number. Each bullet is between sub-tags\\nof either <strength>, <weakness>, or <missinginfo>. A <strength> is where the Answer makes a good point that\\nfollows the Guidelines and contributes to a helpful response. A <weakness> is where the Answer makes an incorrect,\\nirrelevant, unreasonable or overly broad claim that fails to address the full scope of the Question, or otherwise falls\\nshort of the Guidelines. <missinginfo> is when some key event, facts or other material information is omitted but\\nshould have included to strengthen the Candidate Answer.\\n2. Give a binary correctness score indicating whether each Candidate Answer generally agrees with the Reference\\nAnswer – \"<CorrectX>1</CorrectX>\" – or contradicts it \"<CorrectX>0</CorrectX>\", where X is the Candidate\\nAnswer number.\\n3. Explain which Candidate Answer is better and why, i.e. how well it adheres to the Guidelines, between \"<Explana-\\ntion>\" and \"</Explanation>\" tags.\"\\n4. Finally, score each Candidate Answer on 1-100, where 100 is a perfect Answer that aligns with the Guidelines.\\nIndicate the score between \"<ScoreX>\" and \"</ScoreX>\" tags where X is the Candidate Answer number.\\nMake sure to address all {NumCandidates} Candidate Answers. When you are finished, conclude your response with\\n\"=====\".\\n<Critique1>\\nFigure 8: Prompt given to GPT-4 to act as a “critique-then-score” judge of candidate answers side-by-side.\\nResearchy Question: how does public transport help the economy\\nGPT-4 Turbo Closed-book Answer (2023-07-01-preview)\\nPublic transport provides numerous economic benefits to communities and countries as a whole. Here are some of the key ways\\nin which public transportation supports economic growth and efficiency:\\n1. Reduced Congestion Costs: Public transport can significantly reduce the number of vehicles on the road, which in turn\\nreduces traffic congestion. This leads to a more efficient transportation system and increased productivity.\\n2. Cost Savings for Individuals: Public transport can be more cost-effective than owning a private vehicle, leading to\\nincreased disposable income for individuals.\\n3. Job Creation: Public transport systems create jobs for operation, maintenance, and construction, contributing to employ-\\nment opportunities.\\n4. Increased Property Values: Good public transport connectivity often increases property values, leading to higher tax\\nrevenues.\\n5. Environmental Benefits: Public transport reduces the number of cars on the road, decreasing air pollution and greenhouse\\ngas emissions.\\n6. Tourism: Efficient public transport systems enhance tourism by making it easier for tourists to navigate and spend in local\\neconomies.\\n7. Business Productivity: Reliable public transport reduces employee tardiness and absenteeism, increasing business\\nproductivity.\\n8. Social Inclusion: Public transport provides mobility for those without access to private vehicles, allowing for greater\\nworkforce participation and access to services.\\n9. Urban Development: Public transport encourages efficient land use and can be a catalyst for urban revitalization and\\ndevelopment.\\n10. Energy Efficiency: Public transport is more energy-efficient per passenger kilometer than private vehicles, contributing to\\nenergy security.\\nBy providing an affordable and efficient alternative to private vehicles, public transport systems can enhance the overall economic\\nperformance of a region and improve the quality of life for its residents.\\nMixtral 8x7b Closed-book Answer (https://stablediffusion.fr/mixtral)\\nPublic transportation can have a positive impact on the economy in several ways:\\n1. Reduced traffic congestion: Public transportation helps to reduce traffic congestion by providing an alternative to driving,\\nwhich can lead to decreased travel times and increased productivity. This can also reduce the need for costly road expansion\\nprojects.\\n2. Increased economic activity: Public transportation can help to stimulate economic activity by making it easier for people\\nto access jobs, shops, and other services. This can lead to increased spending and investment in local communities.\\n3. Lower transportation costs: Public transportation can provide a more affordable transportation option for many people,\\nespecially those who cannot afford to own and maintain a car. This can help to reduce household transportation costs and\\nfree up money for other expenses.\\n4. Improved air quality: Public transportation can help to reduce air pollution by taking cars off the road. This can lead to\\nimproved public health and reduced healthcare costs.\\n5. Increased property values: Properties located near public transportation hubs, such as train stations and bus stops, often\\nhave higher property values than those located further away. This can lead to increased tax revenues for local governments.\\n6. Job creation: Public transportation agencies and companies create jobs for drivers, mechanics, engineers, and other\\nprofessionals. These jobs can provide stable employment and contribute to the local economy.\\n7. Energy efficiency: Public transportation is generally more energy-efficient than individual car travel. This can help to\\nreduce dependence on fossil fuels and decrease greenhouse gas emissions.\\nOverall, public transportation can play an important role in supporting a strong and sustainable economy by providing affordable,\\nefficient, and environmentally-friendly transportation options.\\nTable 10: Closed-book direct answers from GPT-4 and Mixtra 8x7b (Jiang et al., 2024) to the Researchy Question\\nused in Table 2. We find these two answers to be strikingly similar (suggesting convergent evolution of LLMs), and\\nboth paint in very broad strokes an answer that isn’t “wrong” but rather, superficial.\\nPublic transportation plays a significant role in boosting the economy in several ways:\\n1. Job Creation: Investment in public transportation can create millions of jobs. For instance, every $1 billion invested\\ncould create 50,000 jobs [1,2].\\n2. Economic Returns: Every $1 invested in public transportation could generate $5 in economic returns [1].\\n3. Reduced Traffic Congestion: A well-functioning public transit system reduces traffic congestion, leading to more\\nefficient travel and productivity [3].\\n4. Economic Growth: Public transportation can shape land use and development patterns, stimulate commerce, and\\nincrease property values [4].\\n5. Supports Vulnerable Populations: It improves equality, particularly for the more vulnerable members of society, by\\nproviding them with mobility and access to jobs [1].\\n6. Environmental Impact: Public transit helps improve air quality and reduce greenhouse gas emissions, contributing to\\na sustainable future [3].\\nIn summary, public transportation is more than just a way to move people around. It’s a vehicle for opportunity, equity, and\\na better quality of life.\\nReferences\\n• [1] https://www.weforum.org/agenda/2021/04/here-s-why-cities-should-invest-in-public-transport/\\n• [2] https://infrastructureusa.org/the-economic-impact-of-public-transportation/\\n• [3] https://www.ncesc.com/is-public-transit-better-for-the-economy/\\n• [4] https://www.apta.com/wp-content/uploads/Resources/resources/reportsandpublications/Documents/Economic-\\nRecovery-APTA-White-Paper.pdf\\n• [5] https://profound-tips.com/advice/how-does-public-transportation-help-the-economy/\\nFigure 9: A “screenshot” of the output from Bing Chat in precise mode (a.k.a. Microsoft Copilot) for the question\\n“how does transportation help the economy”.\\nComprehensive Research Report: The Role of Transportation in Economic Development\\n# Introduction Transportation is a critical factor in the economic development of any region or nation. It serves as the\\nbackbone of economic growth, facilitating the movement of goods, services, and labor, which are essential for the efficient\\nfunctioning of the economy. This report synthesizes key points from various sources to illustrate how transportation\\ncontributes to economic prosperity.\\n# Key Contributions of Transportation to the Economy ## Job Creation and Economic Growth Investments in\\ntransportation infrastructure, such as public transit, are directly linked to job creation and economic growth. For every $1\\nbillion invested in transit, an estimated 49,700 to 50,731 jobs can be created, offering a significant return on investment\\nranging from 4 to 1 to as high as 5 to 1 . This investment not only supports American jobs and industry but also generates\\nincome for local businesses and their workers . Moreover, spending on transportation programs creates jobs in the short\\nterm, particularly in construction, and can stimulate the economy .\\n## Enhancing Business Productivity and Competitiveness Transportation improvements lead to reduced travel times\\nand costs, as well as increased network reliability, which in turn enhances business productivity and competitiveness . By\\nreducing transport costs, businesses can access higher-quality or lower-cost inputs and connect with potential suppliers .\\nThis connectivity also allows individuals to access a wider range of jobs, thereby participating more fully in the labor market .\\n## Market Accessibility and Economic Activity An efficient transport system provides market accessibility by linking\\nproducers and consumers, enabling transactions to occur . This linkage is crucial for the economy as transportation\\naccounts for a significant portion of the GDP, ranging from 6% to 12% . By offering cost, time, and reliability advantages,\\ntransportation enables goods to be transported over longer distances, fostering economic changes, most of which are positive .\\n## Induced Investment and Regeneration Impacts Transportation investments can influence the level or location of\\nprivate sector investment due to improved connectivity . These investments can also lead to local economic impacts\\nresulting from an improved local image and the attraction of land use development, known as regeneration impacts .\\n## Structural and Developmental Transformations Sustained improvements in transportation, along with parallel\\nadvancements in information and production technologies and institutional structures, can lead to structural and\\ndevelopmental transformations within the economy . This demonstrates the dynamic nature of transportation’s role in\\neconomic development.\\n## Economic Efficiency and Scale An efficient transportation system reduces costs across many economic sectors, while\\nan inefficient one increases these costs . Moreover, more efficient transport costs less, leading to rising productivity and\\neconomies of scale, which should cause GDP to rise faster than transport activity .\\n## Influence on Economic Development and Military Strength Transportation activities contribute to a nation’s total\\neconomic product and strengthen the national or regional economy . A well-developed transportation network not only\\nmakes markets more competitive but also adds to a nation’s military strength .\\nConclusion Transportation plays a pivotal role in the economic development of a country. It is essential for job creation,\\nenhancing business productivity, providing market accessibility, inducing investment, and contributing to structural\\ntransformations. Governments recognize the importance of investing in transportation infrastructure as a means to stimulate\\neconomic growth and maintain a competitive edge in the global market. As such, transportation is not just a means of\\nmoving people and goods; it is a fundamental driver of economic prosperity.\\nReferences\\n• Economic Impact of Public Transit. (2019, April 22). American Public Transportation Association.\\n• The relationship between transport and the economy. (2017, February 13). Greener Vision.\\n• Economic Impact Of Public Transportation Investment. (2019, March 27). American Public Transportation Associa-\\ntion.\\n• 3.1 – Transportation and Economic Development | The Geography of Transport Systems. (2017, December 4).\\nTransport Geography.\\n• Economic Consequences of Transport Improvements. (2022, October 11). ACCESS Magazine.\\n• Transportation, Jobs, and Economic Growth. (2018, February 14). ACCESS Magazine.\\n• Transportation economics | Definition, Importance, & Facts. (1998, September 8). Britannica.\\n• The role of transport in economic development. FutureLearn.\\nFigure 10: A “screenshot” of the output from YouPro Research engine for the query “how does transportation help\\nthe economy”. https://you.com/search?q=how+does+transportation+help+the+economy\\nQuery #1 - how does transportation help the economy\\nQuery #2 - How does transportation infrastructure affect economic growth?\\nQuery #3 - What are the long-term effects of transportation investments?\\nQuery #4 - How does transportation contribute to GDP?\\nQuery #5 - What are the environmental impacts of transportation?\\nQuery #6 - How does transportation affect urban planning and development?\\nQuery #7 - What are the challenges in financing transportation infrastructure?\\nQuery #8 - How can transportation policy influence economic productivity?\\nFinal Research Result for how does transportation help the economy\\nTransportation plays a pivotal role in the economy, contributing to the movement of goods, services, labor, and capital,\\nwhich are essential for the functioning of a modern society and the growth of an economy [11]. The ability to transport\\ngoods and people efficiently is fundamental to economic life, as seen in the early United States where a robust transportation\\nnetwork was instrumental in the country’s development. In a macroeconomic sense, transportation activities are a part of\\nthe total economic product, and the demand for transportation services contributes to the GDP [2]. The transportation\\nsector not only supports the movement of goods and services but also affects employment, wages, and business income,\\nthereby contributing to the overall economic growth [3].\\nInvestments in transportation infrastructure can lead to job creation and improved connectivity, which in turn, promotes\\neconomic productivity and well-being [22]. Furthermore, efficient transportation systems can reduce poverty by increasing\\naccess to employment, recreational, and other essential life-shaping opportunities [20]. The economic importance of\\ntransportation is also evident in the fact that transportation-related final demand has been increasing since the recession,\\nindicating a strong correlation between transportation and economic activity [8]. Additionally, the World Bank and ITDP\\nreport highlights the importance of active mobility in promoting economic productivity and socially inclusive development\\n[21].\\nTransportation is a critical component of a society, reflecting the aspirations of a society and influencing the location\\ndecisions of people and businesses. It is a vital determinant of economic growth, job creation, and improved living\\nconditions, and its efficiency can lead to a green future by reducing congestion and promoting sustainable development\\n[19].\\nIn conclusion, transportation is not only crucial for the movement of goods and services but also plays a significant\\nrole in the overall economic growth of a country.\\nIt contributes to the GDP, supports employment, and affects\\nwages and business income [2][3]. Investments in transportation infrastructure can lead to job creation and improved\\nconnectivity, promoting economic productivity and well-being [22]. Additionally, transportation plays a vital role\\nin reducing poverty by increasing access to essential opportunities [20]. The economic importance of transportation\\nis evident in the strong correlation between transportation and economic activity [8]. Therefore, transportation is a\\ncritical component of a society, reflecting its aspirations and influencing the location decisions of people and businesses [19].\\nReferences\\n• [2] https://www.britannica.com/money/topic/transportation-economics\\n• [3] https://data.bts.gov/stories/s/Transportation-Economic-Trends-Contribution-of-Tra/smrm-36nv/\\n• [8] https://www.bts.gov/sites/bts.dot.gov/files/docs/browse-statistical-products-and-data/transportation-economic-\\ntrends/215926/transportation-economic-trends-2017-chapter-2.pdf\\n• [11] https://www.nationalgeographic.org/article/effects-transportation-economy/5th-grade/\\n• [19] https://transportgeography.org/contents/chapter3/transportation-and-society/\\n• [20]\\nhttps://blogs.iadb.org/transporte/en/how-can-transport-reduce-poverty-and-promote-socially-inclusive-\\ndevelopment/\\n• [21] https://www.usatoday.com/story/special/contributor-content/2024/01/10/itdp-and-world-bank-report-highlights-\\neconomic-and-environmental-benefits-of-financing-active-mobili/72177281007/\\n• [22] https://www.accessmagazine.org/spring-2011/transportation-jobs-economic-growth/\\nFigure 11: A “screenshot” of the output from SciPhi Research engine for the query “how does transportation help\\nthe economy”. This research engine also asked and separately answered multiple queries before arriving at the final\\nresearch result. https://search.sciphi.ai/research?q=how+does+transportation+help+the+economy\\n\\n\\nAugmenting Researchy Questions with Sub-question Judgments\\nJia-Huei Ju1, Eugene Yang2, Trevor Adriaanse3, Andrew Yates2,\\n1University of Amsterdam,\\n2Human Language Technology Center of Excellence, Johns Hopkins University,\\n3Johns Hopkins University,\\nCorrespondence: j.ju@uva.nl\\nAbstract\\nThe Researchy Questions dataset provides\\nabout 100k question queries with complex in-\\nformation needs that require retrieving infor-\\nmation about several aspects of a topic. Each\\nquery in ResearchyQuestions is associated with\\nsub-questions that were produced by prompting\\nGPT-4. While ResearchyQuestions contains la-\\nbels indicating what documents were clicked\\nafter issuing the query, there are no associations\\nin the dataset between sub-questions and rele-\\nvant documents. In this work, we augment the\\nResearchy Questions dataset with LLM-judged\\nlabels for each sub-question using a Llama3.3\\n70B model. We intend these sub-question la-\\nbels to serve as a resource for training retrieval\\nmodels that better support complex information\\nneeds.\\n1\\nIntroduction\\nResearchy Questions (Rosset et al., 2024) is a col-\\nlection of queries taken from the Bing query log,\\nalong with the URLs that the user clicked, which\\nare also present in the ClueWeb22 dataset (Over-\\nwijk et al., 2022). Such questions are complex and\\nusually require searching with multiple queries in\\na single session to fully answer. In the original\\ndataset, the authors provide the decomposition of\\neach question into several sub-questions by GPT4.\\nHowever, there are no relevance judgments linked\\nto each decomposed sub-questions, which limit the\\nusability of the collection as a sub-question training\\nresource.\\nTo fill the gap, we employ an automatic judg-\\nment model to generate the relevance labels for the\\ntraining sub-questions. We adapt a graded judg-\\nment prompt from CRUX (Ju et al., 2025) to cap-\\nture different granularities of the relevance. Com-\\npared to typical retrieval training collections such\\nas MS MARCO (Bajaj et al., 2016), sub-questions\\nprovided by Researchy Qeustions are less factual\\nand require better semantic understanding to the\\nQwen3-Reranking Instruction\\nSystem prompt: Judge whether the Docu-\\nment meets the requirements based on the\\nQuery and the Instruct provided. Note that\\nthe answer can only be \"yes\" or \"no\".\\nPrompt:\\n<Instruct>:\\nGiven the list of\\nquestions as query, retrieve relevant pas-\\nsages that answer the questions. <Query>:\\n{join(subquestions)} <Document>: {doc}\"\\nFigure 1: The prompt used to rerank documents with\\nthe pointwise Qwen3 0.6B reranking model.\\nquestions and the documents. Such an addition pro-\\nvides a concrete usage for the original Researchy\\nQuestion collection for training factoid question\\nretrieval model.\\n2\\nCandidate Documents\\nTo avoid indexing the entire ClueWeb22 (Over-\\nwijk et al., 2022), which contains more than 10B\\ndocuments, we only consider documents in the\\nEnglish subset of ClueWeb22 Set B, which con-\\ntains 48% of the clicked documents in Researchy\\nQuestions. For efficiency, we retrieved documents\\nfrom ClubWeb22 Set B using BM25. To avoid\\noverly biasing results toward lexical matches, we\\nreranked the top 100 documents with the Qwen3\\n0.6B Reranker (Zhang et al., 2025) using a slightly\\nmodified instruction documented in Figure 1.\\nFor selecting the candidate documents for each\\nsub-question of the Researchy Questions Train\\nsplit, we collect the top 20 documents from the\\ninitial BM25 retrieval results and the top 20 docu-\\nments from the reranking output to balance the\\nlexical and semantic matching in the candidate\\nset.\\nWe additionally include the clicked docu-\\nments (439,161 in total) of the original question\\narXiv:2510.21733v1  [cs.IR]  30 Sep 2025\\nRubric-based Answerability Judgment\\nInstruction: Determine whether the question can be answered based on the provided context? Rate\\nthe context with on a scale from 0 to 5 according to the guideline below. Do not write anything\\nexcept the rating.\\nGuideline:\\n5: The context is highly relevant, complete, and accurate.\\n4: The context is mostly relevant and complete but may have minor gaps or inaccuracies.\\n3: The context is partially relevant and complete, with noticeable gaps or inaccuracies.\\n2: The context has limited relevance and completeness, with significant gaps or inaccuracies.\\n1: The context is minimally relevant or complete, with substantial shortcomings.\\n0: The context is not relevant or complete at all.\\nQuestion: {q}\\nContext: {c}\\nRating:\\nFigure 2: The prompt used to assess the relevance of a document (context) c to a sub-question q on a scale from 0 to\\n5, following prior work obtaining sub-question judgments (Ju et al., 2025).\\nsupplied by the Researchy Question collection to\\nensure they are also processed by our downstream\\npipeline. These documents are then passed to an\\nautomatic judgment model for acquiring relevance\\nlabels, which we describe in the next section.\\n3\\nAutomatic Judgments\\nAfter identifying candidate documents, we used the\\nLlama3.3 70B model to produce relevance judg-\\nments. Each document was automatically judged\\nfor each sub-question derived from the top-level\\nquery. Judgments were on a 0-5 scale using the\\nprompts from from Ju et al. (2025), which are in-\\ntended to assess the answerability (Dietz, 2024;\\nFarzi and Dietz, 2024) of a question given a docu-\\nment. The prompt is shown in Figure 2.\\n4\\nDataset Statistics\\nFor all 1,288,976 sub-questions in the Researchy\\nQuestion training set, we acquire 24,316,320 pre-\\ndictions, which on average have 18.87 judgments\\nper sub-question. Table 1 summarizes the distribu-\\ntion of the documents predicted in each relevance\\nlevel.\\n5\\nConclusion\\nIn this work, we augment the Researchy Question\\ndataset with automatic labels using CRUX. This\\nLevel\\nCount\\nPer Sub-question\\n5\\n1,907,722\\n1.48\\n4\\n5,444,774\\n4.22\\n3\\n2,909,134\\n2.26\\n2\\n7,920,642\\n6.14\\n1\\n2,651,382\\n2.06\\n0\\n3,482,653\\n2.70\\nOther\\n13\\n–\\nTable 1: Statistics of each predicted relevance level.\\nresource will enable modeling exploration for fac-\\ntoid retrieval and question answering. We provide\\nthese generated labels as a community resource for\\nencouraging research in these directions.\\nReferences\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, and 1\\nothers. 2016. MS MARCO: A human generated ma-\\nchine reading comprehension dataset. arXiv preprint\\narXiv:1611.09268.\\nLaura Dietz. 2024. A workbench for autograding re-\\ntrieve/generate systems. In Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research\\nand Development in Information Retrieval, pages\\n1963–1972.\\nNaghmeh Farzi and Laura Dietz. 2024.\\nAn exam-\\nbased evaluation approach beyond traditional rele-\\nvance judgments. arXiv preprint arXiv:2402.00309.\\nJia-Huei Ju, Suzan Verberne, Maarten de Rijke, and An-\\ndrew Yates. 2025. Controlled Retrieval-augmented\\nContext Evaluation for Long-form RAG.\\narXiv\\npreprint arXiv:2506.20051.\\nArnold Overwijk, Chenyan Xiong, Xiao Liu, Cameron\\nVandenBerg, and Jamie Callan. 2022. Clueweb22:\\n10 billion web documents with visual and semantic\\ninformation. arXiv preprint arXiv:2211.15848.\\nCorby Rosset, Ho-Lam Chung, Guanghui Qin, Ethan C\\nChau, Zhuo Feng, Ahmed Awadallah, Jennifer\\nNeville, and Nikhil Rao. 2024.\\nResearchy Ques-\\ntions: A dataset of multi-perspective, decomposi-\\ntional questions for llm web agents. arXiv preprint\\narXiv:2402.17896.\\nYanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang,\\nHuan Lin, Baosong Yang, Pengjun Xie, An Yang,\\nDayiheng Liu, Junyang Lin, Fei Huang, and Jingren\\nZhou. 2025. Qwen3 embedding: Advancing text\\nembedding and reranking through foundation models.\\narXiv preprint arXiv:2506.05176.\\n',\n",
       " 'final_answer': 'Transformer agents refer to advanced AI systems, particularly those employing the transformer architecture, that function interactively within a defined agent loop of thinking, acting, and observing. These agents utilize large language models (LLMs) such as OpenAI\\'s GPT series, which are pre-trained on substantial datasets and fine-tuned for various applications. In recent research, the evolution of transformer agents has focused on enhancing their capabilities through improved interaction with tools, better handling of complex tasks, and boosting their reasoning abilities.\\n\\n### Key Features of Transformer Agents:\\n\\n1. **Agent Loop**: At the core of transformer agents is the agent loop which allows these systems to continuously refine their actions based on outcomes observed from their environment. This cyclical process is crucial for real-time adaptation in diverse and dynamic scenarios, enabling agents to improve performance incrementally.\\n\\n2. **Memory and Planning**: Transformer agents can retain information from past interactions (memory) and make informed plans for future actions. This feature equips them to handle more nuanced queries that require a deeper understanding of context and past inputs.\\n\\n3. **Tool Interaction**: Modern transformer agents are designed to interact with various tools such as web browsers, databases, and APIs. This interaction enables them to retrieve real-time information and perform tasks that extend beyond mere text generation, akin to using specialized software applications.\\n\\n4. **Complex Task Management**: Newer transformer agents are capable of managing open-ended questions that require complex reasoning. Research has introduced datasets like \"Researchy Questions,\" which focus on multi-faceted, non-factoid queries that challenge LLMs like GPT-4 in handling \"unknown unknowns\"—aspects of problems that may not be immediately clear or visible.\\n\\n5. **Decomposition Techniques**: Recent advancements in processing complex questions involve breaking them down into sub-questions. Techniques such as chain-of-thought reasoning are employed, where the AI considers how to reach the answer step-by-step rather than attempting to solve it in one go. This method enhances the accuracy and depth of responses.\\n\\n6. **Evaluating Agent Performance**: Research has also shifted towards evaluating transformer agents based on their ability to generate accurate responses to complex queries, with benchmarks being developed that assess reasoning, factuality, and alignment with human expectations. Models are trained using methods such as Reinforcement Learning from Human Feedback (RLHF) to ensure that responses meet user needs effectively.\\n\\n### Recent Research Directions:\\n\\n- **Emergence of Researchy Questions**: The \"Researchy Questions\" dataset has emerged as essential for evaluating how transformer agents can decompose complex queries into manageable parts. Designed to challenge AI systems, it encourages the development of techniques that enable deeper understanding and response synthesis across diverse domains.\\n\\n- **Agentic AI Frameworks**: The introduction of frameworks that allow LLMs to function as agents in iterative cycles has led to the development of tools that mimic human-like interaction patterns with information retrieval systems.\\n\\n- **Real-World Application Integration**: Many contemporary studies are exploring how transformer agents can be leveraged in practical applications, such as enhancing customer service, conducting research, and supporting decision-making processes through effective AI-human collaboration.\\n\\nIn summary, transformer agents are evolving rapidly, employing complex feedback loops, improved reasoning capabilities, and enhanced interaction with external tools. Ongoing research focuses on addressing their limitations in real-world applications and refining their ability to solve intricate problems through advanced data structures and cognitive techniques.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425788d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
